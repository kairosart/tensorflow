{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.694511\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.584385\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 61.9%\n",
      "Minibatch loss at step 100: 1.191001\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 150: 0.704988\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 200: 0.566068\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 250: 1.337007\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 300: 0.675867\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 350: 0.326337\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 400: 0.708448\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 450: 0.437135\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 500: 0.592598\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 550: 0.442888\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 600: 0.492404\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 650: 0.959696\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 700: 0.680489\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 750: 0.549372\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 800: 0.047278\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 850: 0.663060\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 900: 0.399496\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 950: 0.350953\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1000: 0.325043\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model Accuracy\n",
    "\n",
    "We'll plot a graph with the accuracy results of each model at the end of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methodDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # Convolution 1   \n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # Pooling 1\n",
    "    pool1 = tf.nn.max_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # Convolution 2\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    bias2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    \n",
    "    # Pooling 2\n",
    "    pool2 = tf.nn.max_pool(bias2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.940207\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.900450\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 41.8%\n",
      "Minibatch loss at step 100: 1.383264\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 150: 0.786751\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 200: 0.362062\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 250: 1.302935\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 300: 0.518037\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 350: 0.357974\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 400: 0.846707\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 450: 0.334236\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 500: 0.540606\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 550: 0.417764\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 600: 0.324741\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 650: 1.072085\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 700: 0.488988\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 750: 0.652625\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 800: 0.122137\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 850: 0.408636\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 900: 0.400549\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 950: 0.301657\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.292556\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.0%\n",
      "Test accuracy: 90.5%\n",
      "{'Max pooling model': 90.51}\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Max pooling model'] = accuracy(test_prediction.eval(), test_labels)\n",
    "print(methodDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  size3 = ((image_size - patch_size + 1) // 2 - patch_size + 1) // 2\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [size3 * size3 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # C1 input 28 x 28\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # S2 input 24 x 24\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # C3 input 12 x 12\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    # S4 input 8 x 8\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # F6 input 4 x 4\n",
    "    shape = pool4.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.192108\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.045614\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 42.0%\n",
      "Minibatch loss at step 100: 1.542213\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 51.5%\n",
      "Minibatch loss at step 150: 1.188119\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 200: 0.810712\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 250: 1.489165\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 300: 0.826287\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 350: 0.393781\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 400: 0.894209\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 450: 0.512233\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 500: 0.663622\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 550: 0.620105\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 600: 0.320397\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 650: 1.080140\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 700: 0.763104\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 750: 0.636939\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 800: 0.142734\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 850: 0.495264\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 900: 0.460357\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 950: 0.297554\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 0.430946\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1050: 0.645618\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1100: 0.517345\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1150: 0.422705\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1200: 0.583516\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1250: 0.323690\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1300: 0.623659\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1350: 0.496995\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1400: 1.071670\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1450: 0.753293\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1500: 0.042789\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1550: 0.175861\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1600: 0.562243\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1650: 0.345833\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1700: 0.373911\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 1750: 0.475095\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1800: 0.953472\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1850: 0.736491\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1900: 0.369137\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1950: 0.521440\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.375771\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2050: 1.123352\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2100: 0.510760\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2150: 0.625763\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2200: 0.586676\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2250: 0.197258\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2300: 0.073691\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2350: 0.658607\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2400: 0.254058\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2450: 0.441322\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 0.334571\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2550: 0.811867\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2600: 0.409443\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2650: 0.092696\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2700: 0.544685\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2750: 0.512564\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2800: 0.846284\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2850: 0.659449\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2900: 0.143423\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2950: 0.346978\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 0.459014\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3050: 0.885636\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3100: 0.846252\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3150: 0.396816\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3200: 0.663417\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3250: 0.628549\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3300: 0.484761\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3350: 0.238209\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3400: 0.233142\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3450: 0.133743\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3500: 0.430026\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3550: 0.652752\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3600: 0.437259\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3650: 0.231508\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3700: 0.171573\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 3750: 0.054232\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3800: 0.478608\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3850: 0.102965\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3900: 0.551394\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3950: 0.851359\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 4000: 0.451451\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4050: 0.118685\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4100: 0.262647\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4150: 1.008011\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4200: 0.405171\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4250: 0.442683\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4300: 0.449721\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4350: 0.534603\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4400: 0.236055\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4450: 0.547767\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4500: 0.099484\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4550: 0.164890\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4600: 0.775262\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4650: 0.325414\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4700: 0.141528\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4750: 0.220201\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4800: 0.177094\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4850: 0.087459\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4900: 1.601711\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4950: 0.321858\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.213441\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 5050: 0.243240\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5100: 0.335324\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 5150: 0.515685\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 5200: 1.315915\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 5250: 0.614797\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5300: 0.452238\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5350: 0.868882\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 5400: 0.367896\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 5450: 0.010330\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 5500: 0.518447\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 5550: 0.952747\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5600: 0.526211\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 5650: 0.051487\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5700: 0.777706\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5750: 0.483540\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 5800: 0.090037\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5850: 0.453121\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5900: 1.000663\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5950: 1.441727\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 6000: 0.328419\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 6050: 0.309734\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6100: 0.165188\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6150: 0.186340\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6200: 0.405751\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 6250: 0.623293\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6300: 0.526068\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 6350: 0.758402\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6400: 0.615852\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6450: 0.341796\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 6500: 0.526368\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6550: 0.458308\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6600: 0.652566\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 6650: 0.173319\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 6700: 0.440885\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6750: 0.051318\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6800: 0.218282\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 6850: 0.165381\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 6900: 0.405981\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 6950: 0.223817\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 7000: 0.712334\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7050: 0.398046\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7100: 0.296056\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 7150: 0.796190\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7200: 0.904976\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7250: 0.258290\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7300: 0.510422\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7350: 0.863629\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 7400: 0.565562\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 7450: 0.523810\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7500: 0.414010\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7550: 0.423968\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7600: 0.612535\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7650: 0.196631\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7700: 0.840128\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 7750: 0.150544\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7800: 0.529589\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7850: 0.420494\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7900: 0.856908\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7950: 0.785829\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8000: 0.149160\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8050: 0.507719\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 8100: 0.684982\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 8150: 0.703819\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8200: 0.189595\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8250: 0.649881\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8300: 0.523840\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8350: 0.206339\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8400: 0.437522\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 8450: 0.561672\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8500: 0.085331\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8550: 0.447121\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 8600: 0.399222\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8650: 0.258899\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8700: 0.147212\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 8750: 0.773419\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 8800: 0.402960\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 8850: 0.487632\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8900: 0.333877\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 8950: 0.334921\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 9000: 0.228265\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9050: 0.317924\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9100: 0.766595\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9150: 0.587094\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9200: 0.441224\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9250: 0.329079\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9300: 0.572631\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9350: 0.432822\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9400: 0.228520\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 9450: 0.213685\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9500: 0.329635\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9550: 0.379480\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9600: 0.363009\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 9650: 0.117791\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 9700: 0.245511\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9750: 0.350666\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9800: 0.277012\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9850: 0.138220\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9900: 0.402479\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 9950: 0.366327\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10000: 0.262867\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 10050: 0.120480\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 10100: 0.201538\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10150: 0.277423\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 10200: 0.975803\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10250: 0.257829\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10300: 0.339166\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10350: 0.220660\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 10400: 0.472711\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10450: 0.472964\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 10500: 0.342817\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 10550: 0.269335\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 10600: 0.183698\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 10650: 0.090068\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 10700: 0.319260\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 10750: 0.729927\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 10800: 0.042227\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10850: 0.603645\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 10900: 0.498072\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 10950: 0.308018\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 11000: 0.195816\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11050: 0.181731\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 11100: 1.085994\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 11150: 0.543041\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 11200: 0.240206\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11250: 0.360117\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11300: 0.410923\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 11350: 0.261008\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11400: 0.389937\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11450: 0.167422\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11500: 0.210633\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 11550: 0.198388\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11600: 0.319207\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 11650: 0.475999\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11700: 0.712742\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 11750: 0.744724\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11800: 0.305261\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 11850: 0.879175\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11900: 0.244519\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 11950: 0.694807\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12000: 0.663355\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12050: 0.297027\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12100: 0.145441\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12150: 0.263336\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12200: 0.551998\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 12250: 0.056306\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12300: 0.457170\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12350: 0.326967\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12400: 0.523178\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 12450: 0.291856\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12500: 0.027292\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12550: 0.903802\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 12600: 0.147376\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12650: 0.259308\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12700: 0.162552\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12750: 0.380548\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12800: 0.514446\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12850: 0.140876\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12900: 0.141647\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 12950: 0.207610\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13000: 0.387379\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13050: 0.313250\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13100: 0.237908\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13150: 0.140818\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 13200: 0.150213\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13250: 0.117713\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 13300: 0.192598\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13350: 0.104631\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13400: 0.225886\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13450: 0.613383\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13500: 0.076719\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13550: 0.405128\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13600: 0.138672\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13650: 0.115794\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 13700: 0.442054\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13750: 0.069809\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13800: 0.153691\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13850: 0.168422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13900: 0.062261\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 13950: 0.491039\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14000: 1.181628\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14050: 0.605076\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14100: 0.343221\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14150: 0.120087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 14200: 0.495939\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14250: 0.508769\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14300: 0.137437\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14350: 0.224952\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14400: 0.281054\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14450: 0.699214\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14500: 0.028813\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 14550: 0.390101\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14600: 0.167087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14650: 0.306587\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14700: 0.373931\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 14750: 0.205574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14800: 0.579190\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14850: 0.463960\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14900: 0.197299\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14950: 0.173291\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 15000: 0.496397\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 15050: 0.181784\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 15100: 0.223186\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15150: 0.387905\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 15200: 0.309049\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15250: 0.443893\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 15300: 0.495869\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 15350: 0.315753\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 15400: 0.361177\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 15450: 0.319592\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 15500: 0.292488\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15550: 0.548299\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 15600: 0.368066\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 15650: 0.207790\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15700: 0.242041\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 15750: 0.394381\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15800: 0.260559\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15850: 0.757582\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15900: 0.476658\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 15950: 0.377029\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 16000: 0.463498\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16050: 0.914703\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16100: 0.188132\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16150: 0.539043\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 16200: 0.368751\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16250: 0.770836\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16300: 0.431049\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16350: 0.511099\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 16400: 0.078548\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16450: 0.166847\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 16500: 0.176990\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16550: 0.027639\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 16600: 0.078315\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 16650: 0.037232\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16700: 0.444165\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16750: 0.202858\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 16800: 0.108791\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 16850: 0.575708\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16900: 0.136319\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 16950: 0.511343\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17000: 0.378779\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 17050: 0.246594\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17100: 0.265573\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 17150: 0.427656\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 17200: 0.115421\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17250: 0.224614\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 17300: 0.295323\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 17350: 0.556929\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 17400: 0.288889\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17450: 0.422204\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17500: 0.143617\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 17550: 0.271179\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17600: 0.318833\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17650: 0.218967\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 17700: 0.191087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 17750: 0.672232\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 17800: 0.514908\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17850: 0.892964\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 17900: 0.189286\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 17950: 0.347452\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 18000: 0.711434\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 18050: 0.008049\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 18100: 0.689129\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 18150: 0.018422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 18200: 0.093089\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18250: 0.674439\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 18300: 0.405817\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18350: 0.053010\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 18400: 0.246423\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 18450: 0.410607\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18500: 0.065634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 18550: 0.320134\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18600: 0.333310\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 18650: 0.324293\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18700: 0.051870\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 18750: 0.259359\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 18800: 0.495767\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18850: 0.581791\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 18900: 1.035940\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18950: 0.855857\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19000: 0.361122\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19050: 0.358722\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19100: 0.419394\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19150: 0.181066\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19200: 0.441145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 19250: 0.306502\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19300: 0.358956\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19350: 0.349712\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19400: 0.689344\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19450: 0.449081\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 19500: 0.255452\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19550: 0.227848\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19600: 0.517874\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 19650: 0.204076\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19700: 0.099796\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19750: 0.347965\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 19800: 0.976077\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19850: 0.506109\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 19900: 0.863012\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 19950: 0.090336\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 20000: 0.088749\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 95.0%\n",
      "{'LetNet5 model': 95.04, 'Max pooling model': 90.51}\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['LetNet5 model'] = accuracy(test_prediction.eval(), test_labels)\n",
    "print(methodDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dropout and learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "beta_regul = 1e-3\n",
    "drop_out = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  size3 = ((image_size - patch_size + 1) // 2 - patch_size + 1) // 2\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [size3 * size3 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob):\n",
    "    # C1 input 28 x 28\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # S2 input 24 x 24\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # C3 input 12 x 12\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    # S4 input 8 x 8\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # F5 input 4 x 4\n",
    "    shape = pool4.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden5 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # F6\n",
    "    drop5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "    hidden6 = tf.nn.relu(tf.matmul(hidden5, layer4_weights) + layer4_biases)\n",
    "    drop6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "    return tf.matmul(drop6, layer5_weights) + layer5_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, drop_out)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.85, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.808042\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.084415\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 29.6%\n",
      "Minibatch loss at step 100: 1.693006\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 43.9%\n",
      "Minibatch loss at step 150: 1.228538\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 55.5%\n",
      "Minibatch loss at step 200: 1.207803\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 250: 1.562246\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 62.8%\n",
      "Minibatch loss at step 300: 1.008751\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 350: 0.872817\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 400: 1.073565\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 450: 0.640147\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 500: 0.961452\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 550: 0.919875\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 600: 0.460568\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 650: 1.137080\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 700: 0.847546\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 750: 1.005122\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 800: 0.342330\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 850: 0.766897\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 900: 0.770453\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 950: 0.606284\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1000: 0.811160\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1050: 0.780041\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1100: 0.786229\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1150: 0.516455\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1200: 0.811172\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1250: 0.470059\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1300: 0.894480\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1350: 0.866817\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1400: 0.822968\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1450: 1.295877\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1500: 0.349068\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1550: 0.431376\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1600: 0.888920\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1650: 0.336517\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1700: 0.332970\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1750: 0.884090\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1800: 1.011695\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1850: 0.922131\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1900: 0.407888\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1950: 0.576621\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2000: 0.868773\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2050: 1.010728\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2100: 0.535358\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2150: 0.511041\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2200: 0.475398\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2250: 0.301635\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2300: 0.122205\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2350: 0.562082\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2400: 0.236483\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2450: 0.705108\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2500: 0.471796\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2550: 0.665546\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2600: 0.587240\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2650: 0.104892\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2700: 0.702387\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2750: 0.565855\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2800: 0.465884\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2850: 0.705785\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2900: 0.400671\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2950: 0.330780\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3000: 0.458984\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 3050: 0.653489\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 3100: 0.989245\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3150: 0.594449\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3200: 0.867422\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3250: 0.463325\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3300: 0.773962\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3350: 0.217412\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3400: 0.529611\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 3450: 0.330428\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3500: 0.617721\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3550: 0.629787\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3600: 0.470892\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3650: 0.222395\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3700: 0.333403\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 3750: 0.093919\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3800: 0.511055\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3850: 0.160441\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3900: 1.041390\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3950: 0.763686\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4000: 0.378076\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4050: 0.230048\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 4100: 0.423808\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 4150: 1.139772\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 4200: 0.609440\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 4250: 0.435079\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4300: 0.580457\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 4350: 0.325854\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4400: 0.264700\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 4450: 0.475646\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 4500: 0.241386\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4550: 0.213913\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4600: 0.978335\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 4650: 0.682882\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4700: 0.212994\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 4750: 0.298267\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 4800: 0.292156\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4850: 0.142997\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 4900: 1.134844\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 4950: 0.429855\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 5000: 0.466610\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.0%\n",
      "Test accuracy: 91.8%\n",
      "{'LetNet5 model': 95.04, 'Max pooling model': 90.51, 'Dropout and L. Decay model': 91.8}\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Dropout and L. Decay model'] = accuracy(test_prediction.eval(), test_labels)\n",
    "print(methodDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting method success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAIICAYAAACl5sKnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1wAAsIi4BgCARcQ1AAAsIq4BAGCRLzZWKJ5XXB/AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f433d1c9cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def plotSuccess():\n",
    "    s = pd.Series(methodDict)\n",
    "    s.sort()\n",
    "    \n",
    "    # Colors\n",
    "    ax = s.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    plt.ylim([90.0, 100.0])\n",
    "    plt.xlim([-0.5, 3])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Success of methods')\n",
    "     \n",
    "    plt.show()\n",
    "plotSuccess()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We've got the best score with LetNet5 aplying Dropout and Learning Rate Decay. So we've haven't gotten 97%. Maybe changing some parameters it will perform better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
