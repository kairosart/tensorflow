{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Accuracy\n",
    "\n",
    "We'll plot a graph with the accuracy of each model at the end of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methodDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-13d948a74b29>:31: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 14.010277\n",
      "Training accuracy: 12.6%\n",
      "Validation accuracy: 15.5%\n",
      "Loss at step 100: 2.269790\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 71.7%\n",
      "Loss at step 200: 1.842049\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 300: 1.606454\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 400: 1.443863\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 500: 1.321801\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.6%\n",
      "Loss at step 600: 1.225496\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 75.8%\n",
      "Loss at step 700: 1.147396\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 76.0%\n",
      "Loss at step 800: 1.082737\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 76.1%\n",
      "Test accuracy: 82.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Simple Gradient Descent'] = accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 18.811823\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 500: 1.176309\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1000: 1.411934\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1500: 1.118495\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 2000: 0.974265\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 1.200171\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 3000: 1.175749\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.8%\n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Stochastic Gradient Descent'] = accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#SGD with relu\n",
    "batch_size = 128\n",
    "relu_count = 1024 #hidden nodes count\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, relu_count]))\n",
    "    biases_1 = tf.Variable(tf.zeros([relu_count]))\n",
    "    \n",
    "    # send relu to final nn layer\n",
    "    weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([relu_count, num_labels]))\n",
    "\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation. (#layer_1 -> layer_2(relu) -> layer_3)\n",
    "    logits = tf.matmul( tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1), weights_2) + biases_2\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul( tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul( tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 288.102539\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 35.0%\n",
      "Minibatch loss at step 500: 8.705494\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 21.455761\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1500: 9.838093\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 3.963833\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2500: 2.694866\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 5.263715\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3500: 1.666757\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 4000: 2.398194\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 4500: 0.525604\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 83.8%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['SGD 1-hidden layer'] = accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plotting method success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuAAAAIICAYAAAAvyVvmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0hBZgEuSJElD9H84nnXFu1KqygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe01636390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def plotSuccess():\n",
    "    s = pd.Series(methodDict)\n",
    "    \n",
    "    \n",
    "    # Colors\n",
    "    ax = s.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    plt.ylim([80.0, 100.0])\n",
    "    plt.xlim([-0.5, 3])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Success of methods')\n",
    "     \n",
    "    plt.show()\n",
    "plotSuccess()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
