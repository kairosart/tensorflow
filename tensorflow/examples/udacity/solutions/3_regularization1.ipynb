{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model Accuracy for showing at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "methodDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.045174\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 500: 2.514117\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1000: 1.762749\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 1500: 1.268241\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2000: 0.878130\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2500: 0.944638\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 1.015705\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "\n",
    "      losses.append(l)\n",
    "      acc.append(accuracy(predictions, batch_labels))\n",
    "      valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    " \n",
    "  methodDict['Logistic model'] = accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting loss and accuracy by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAHjCAYAAAAdc7jLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGIfNynKYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb3a88cad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAHjCAYAAADlk0M8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl4VPd97/HPT/sGkhAgFiEkFhvbEIwRGAwGbV5oXDtN\nk9Rp7o3duMH0yd7mtmmb3jRPnLRu0yW96Y1M7CSkdUKc5CZ23GLHZ0AGB28IsLGNbfAMYhcCsUkC\nbfO7f2iksEhiRsyZc0Z6v55Hj2bV+cAPJx8O3zk/Y60VAAAAgOileB0AAAAASDaUaAAAACBGlGgA\nAAAgRpRoAAAAIEaUaAAAACBGlGgAAAAgRpRoAAAAIEaUaAAAACBGlGgAAAAgRmleB4jG+PHjbVlZ\nWcKP29bWptzc3IQfF0NjXfyHNfEn1sV/WBN/Yl38x6s1aWhoOG6tnRDNa5OiRJeVlWnbtm0JP259\nfb0qKysTflwMjXXxH9bEn1gX/2FN/Il18R+v1sQY0xjtaxnnAAAAAGJEiQYAAABiRIkGAAAAYkSJ\nBgAAAGJEiQYAAABiRIkGAAAAYkSJBgAAAGJEiQYAAABiRIkGAAAAYkSJBgAAAGJEiQYAAABiRIkG\nAAAAYkSJBgAAAGJEiQYAAABiRIkGAAAAYkSJBgAAAGJEiQaAEaiju0NhG/Y6BgCMWGleBwAAxEfY\nhrUptEl1DXX65du/VDgcVtG2Ik3InaDxOeM1IWdC71du7/fxOeP7b/e9JiM1w+tfBgAkBUo0ACS5\n4+3HtW7nOj3S8Ij2tOxRUXaRPrXoU2o50qLcCblqbm9Wc3uz3mp+S83tzTrRfkJWdsCfNTZz7OVF\ne4jinZueK2NMgn/FAOA9SjQAJCFrrbYe2Kq6hjr99M2fqqOnQ8tLl+srK7+i37/+95WVlqX6+npV\nVlZe9t6ecI9Onj+p5rbect3c1qzj7cf7bze3994/cOaAth/Zrub2ZnX2dA6YIysta+CifcH9C8+E\nF2YXKsUwSQgg+VGiASCJnD5/Wv/x+n+oblud3mx+U2Mzx+qTN31SD1Y8qLkT50b1M1JTUjU+Z7zG\n54zXdbruiq+31ups59neoj1Q8b6gfO9t2avmtmad7Tw78LFNqopyii4/qz3AWe4JORNUlFPEiAkA\nX6JEA0AS2HZ4m+q21enHb/xY7V3tqphSoUd/91HdO/de5WbkunpsY4zGZo7V2MyxmlE4I6r3nO8+\nr+Ptxy8r3n1nufvuv3HsDTW3NavlXMugIyb5mflRneXue8zt3w8AkCjRAOBbbZ1t+vEbP1bdtjo1\nHGlQTnqO/nDuH+rBigdVMaXC63hDykrLUsnYEpWMLYnq9T3hHrWcaxl0vKSvfO8/vV8NRxrU3Nas\nrnDXgD8rOy378g9NZl9+lrvvNQVZBYyYAIgZJRoAfGZX0y490vCI/uP1/9CZjjOaO3Gu/v13/l0f\nm/cx5Wflex3PFakpqb3lNneCNOHKr+8bMRnwLPclxfvdE++qub1ZrZ2tAx/b/Ha85cLxkoHOcve9\nLj01Pc6/AwCSDSUaAHzgfPd5/fTNn6quoU5bD2xVZmqmPnLDR7SmYo2WlizlChiXuHDEZOa4mVG9\n53z3+YHPcl8y3/160+s63n58yBGTgqyCy4v2IMV7Qu4E5aTnxPOXD8AHKNEA4KF3T7yrR7Y9oh+8\n9gO1nGvR7HGz9U+3/5Pum3+finKKvI43omSlZWla/jRNy58W1eu7w929IyYDneVua9bxc733953a\np1cPvarj7ceHHDGZkDtB6d3pKg4WKyc9RznpOcpNz73oe056jnIzci9/fpDHstKyGEUBPEKJBoAE\n6+zp1JNvP6m6hjptDG1UWkqafm/O72lNxRpVlVVx1tkn0lLSNDF3oibmTozq9dZanek4M+R4yd6D\ne5WTnqP2rnY1tzWrvatd7V3tautqU3tX+6CXEhxKdlp2f8m+Yhkf4PkrvScjNYM/k8AAKNEAkCD7\nTu3Tdxu+q8d2PKamtiZNz5+ur1d/XZ9Y8AlNypvkdTxcJWOM8rPylZ+Vr1njZg34msGu3d2nO9z9\n22Ld2XZRwY7psa42tZxr0cEzBy97Ptbt4FNNalTF+6LHoizrfY+lpqTGlAnwA0o0ALioJ9yj/97z\n36prqNOGPRtkjNFd19ylNQvX6PaZt1MecJG0lLT+WW83WGvV2dPZX6j7CvaFJXvIx7ovfv5I65HL\n3nOu+1zMuTJSM4YcXYlmtGWoAp+dls3ZdMSdqyXaGPMFSX8syUraJemPJE2WtF5SkaQGSf/TWhv7\nv19h1AnbsP7s2T9T85Fmpc1I05KSJUpL4e+B8KdDZw7psR2P6dHtj+rAmQOanDdZf7Pib/THN/1x\n1DO5QLwZY5SZlqnMtEyNyx7nyjHCNqxzXeeGdwa9s03t3b+93drZqqa2psveM9js+VAGK955GXky\nrUZbU7dqev50TS+Yrun50zVlzBT+koshudZAjDFTJX1W0vXW2nPGmCck3SvpdyT9i7V2vTGmTtID\nkr7jVg6MHLuadulfX/5XSdLj339cBVkFum3Gbbpz1p26c9admjJmiscJMdqFbVhO0FHdtjo99c5T\n6rE9un3m7frWnd/SXdfcxWXRMCqkmBTlZuS6uulNV09XVGfQhxp36XvuyNkjCp0I6b82/tdFx0hL\nSVPJ2BKVFZT1lusLCnZZQZmm5U9jN81Rzu3TeGmSso0xXZJyJB2RVC3pDyPPr5P0t6JEIwqBUECS\n9L2K72lM+Rht2LNBz7z3jH761k8lSfOL5+vOWXdq1axVumXaLRQWJExzW7O+v/P7eqThEQVPBjU+\nZ7z+bOmfafXC1VFffg1A9NJT05Wfmh+366bX19dr8bLF2n96v/ad2qfGU41qPB35OtWoQCigQ2cO\nXXTJQyOjyWMmX1auL7zP7pkjm2sl2lp7yBjzTUn7JZ2T9Gv1jm+cstZ2R152UNJUtzJgZHGCjuaM\nn6Py3HJVXl+pD13/IVlrtevYLm3Ys0Eb9m7QP734T3r4Nw9rbOZY1c6o1Z0z79Sq2aui3jUNiJa1\nVlv2b1Hdtjr9fPfP1dnTqZXTV+rr1V/X7835PWWmZXodEUAMctJzNGf8HM0ZP2fA5zt7OnXwzMHf\nFuzI932n9umVQ6/o52/9/LIxk6LsIk0vmD7o2eyCrAJmtZOYsXbgC8lf9Q82plDSzyX9gaRTkn4q\n6WeS/tZaOyvymmmSNlhr5w7w/tWSVktScXHxwvXr17uScyitra3Ky8tL+HFxua5wl+7+zd1aNWmV\nPjH5E4OuS1t3m7af2q6XW17WKy2vqLmjWZJUnluuxYWLtXjcYs3Ln6f0FM5Sx9No+m/lbNdZPdv0\nrJ4+8rQa2xuVl5anO4rv0O9O/l1Nz53udbyLjKZ1SRasiT/FY116bI9aOlvUdL5JR88fVVNHk5rO\nR74it8+Hz1/0npzUHBVnFas4s7j/+6SsSSrO6v1emF44aku2V/+tVFVVNVhrK6J5rZsl+sOS7rTW\nPhC5/3FJSyV9WNIka223MWapekv1HUP9rIqKCrtt2zZXcg7lSpciQuJsbtyslT9YqV/+wS+VfzQ/\nqnWx1urN5jf1zN5ntGHvBm1p3KKucJfyMvJUU16jVbNW6c5Zd2p6gb+KTzIa6f+tWGv1yqFXVNdQ\np/VvrNf57vO6eerNWlOxRh+54SO+3Y1upK9LMmJN/CkR62Kt1YlzJy4eF7ngbHbj6UadOn/qovdk\npmaqNL+092x2fln/Wey+71PHTh2xH7D36r8VY0zUJdrN3/n9kpYYY3LUO85RI2mbpE2SPqTeK3Tc\nJ+lJFzNghHCCjlJMilaWrdTOozujeo8xRnMnztXciXP1xVu+qLMdZ7UxtLG/VD/5Tu8fvevGX6dV\ns1Zp1exVurX0Vv4ZHv3OdpzVj3b9SHUNddp5dKfyMvJ03/z79ODCB7Vg8gKv4wFIIsYYjc8Zr/E5\n41UxZeCOdqbjzEUFu69cN55u1K/e/ZWa2pouen2qSVXJ2JLflusBPvyYlZaViF/eqOTmTPTLxpif\nSdouqVvSDklrJf2XpPXGmIcijz3mVgaMHIFQQIumLFJBVsGwf8aYzDG6Z849umfOPbLW6u3jb2vD\n3g16Zu8z+var39Y/v/TPyknPUXV5dW+pnrVK5YXlcfxVIFm8dvQ11W2r03/u+k+1drZqfvF8fef9\n39HH5n1MYzLHeB0PwAg1NnOs5hXP07zieQM+f67rnPaf3n/RWey+2/X76nXo7KHLNtOZlDepv1wP\ndDab/00bPlf/DcBa+xVJX7nk4aCkxW4eFyPLmY4zevngy/rS8i/F7WcaY3TdhOt03YTr9KdL/1Rt\nnW3atG9T/1nqp999WpJ0TdE1/YV6ZdlK/kY/gp3rOqcn3nxCdQ11eungS8pKy9K9c+/Vgwsf1M1T\nbx61c4kA/CM7PVvXjr9W146/dsDnu3q6dOjsoQHPZm8/sl2/fPuXl20tX5hV2PvBx0HOZo/LHsf/\n/g1iZA7SYER5ft/z6rE9qp1R69oxcjNyddc1d+mua+6StVZ7Wvb0X0LvkYZH9K2Xv6XstGxVllX2\nj34Mtq0vksvbx9/WI9se0Q9e+4FOnT+lOePn6F/v+Fd9fP7HVZhd6HU8AIhaemq6ygrKVFZQNuDz\nYRtWU2vTb+ewLzibvefEHjlBR62drRe9Jzc997KCfeGl/CblTVKKSUnAr85/KNHwPSfoKDstW0tL\nlibkeMYYXVN0ja4pukafW/I5tXe16/l9z/ePfnz2mc9Kz0izxs3qv4ReZVmlbz9chst19nTqF7t/\nobqGOtXvq1d6Srp+//rf15qFa7Ri+grOugAYkVJMiiaPmazJYyZrScmSy5631qrlXMvF4yKnGrXv\ndG/hfvnQy2o513LRezJSMzRt7LTLrpHd971kbMmI3beBEg3fc0KOVkxf4dkH/nLSc7Rqdu/ZZ0l6\nr+U9bdjbe13qx3Y8pm+/+m1lpmaqsqyyf7OXa4quoYj5UPBkUGsb1up7O76n5vZmlReU6+9r/l5/\ntOCPNDF3otfxAMBTxhgV5RSpKKdIN02+acDXnO042z+XfenZ7A17N+hI65GLXp9iUjR1zNRBz2aX\n5pcqOz07Eb+8uKNEw9cOnz2st5rf0v3z7/c6Sr+Z42bq04s/rU8v/rTOd5/X5sbN/Zu9fOHZL+gL\nz35B5QXl/ZfQqy6vZtcqD3WHu/X0u0+rbludnn3vWaWaVN197d16cOGDum3mbaP2nyEBYDjGZI7R\nDRNv0A0Tbxjw+fPd53Xg9IEBP/z4wv4XtP7MevXYnoveMzF34mUffuw+1a1KVSbgVzR8lGj42sbQ\nRklydR76amSlZen2mbfr9pm361/0LwqdDPV/OHHda+v0f7f9X2WkZmjF9BX9ox/Xjb+Os9QJcPDM\nQT26/VF9d/t3dfjsYZWMLdFXK7+qBxY8oKlj2SgVANyQlZal2UWzNbto9oDPd4e7dfjs4YuvkR25\n/XrT6/rVO79SR0+HFhYs1Of1+QSnjw0lGr7mBB0VZRdp/qT5XkeJSnlhuf5k0Z/oTxb9iTq6O/TC\n/hf6Rz+++NwX9cXnvqjS/NL+s9Q15TVcXiiOwjasX7/3a9Vtq9Ov3v2VrLW6c9ad+s77v6Pfmf07\nI3ZTAgBIFmkpaSrNL1Vpfqlu1a2XPW+t1bG2Y9q4ZaMH6WLD/6PAt6y1coKOambUJOU/uWemZapm\nRo1qZtTom7d/U/tP7+8/S/34rsf1SMMjSk9J1/LS5f2leu7EuZylHoam1iZ9b8f3tHb7Wu07tU8T\ncyfqL5b9hT550ye51jcAJBFjjIrzijU5e7LXUa6IEg3feufEOzp09pBqy/05yhGr0vxSrV64WqsX\nrlZnT6e2HtjaP0v9586f68+dP1fJ2JL+sY+a8hrlZ+V7Hdu3rLWq31evuoY6/WL3L9QV7lJ1ebUe\nrn1YH5jzAWWkZngdEQAwglGi4VtO0JEk1cyo8ThJ/GWkZqiyrFKVZZV6+LaHdfDMQT2791lt2LtB\nT7z1hB7d8ajSUtJ0y7Rb+jd7eV/x+zhLLanlXIvW7VynRxoe0Tsn3lFhVqE+s/gzWr1w9aAbEAAA\nEG+UaPhWIBRQeUG5ZhTO8DqK60rGluiBmx7QAzc9oK6eLr148MX+0Y+/DPyl/jLwl5qcN7n/Enq1\nM2pH1UYg1lq9dPAl1TXU6Sdv/EQdPR26Zdot+uGtP9SHrv9Q0l4eCQCQvCjR8KXucLc2hTbpIzd8\nxOsoCZeemq4V01doxfQV+kbNN3Tk7BE9+17vWepfvP0LfX/n95VqUrWkZEn/7ok3TroxKefGr+RM\nxxk9/vrjqmuo0+tNr2tMxhg9sOABPVjxoN5X/D6v4wEARjFKNHyp4XCDTnec9u2l7RJp8pjJuv/G\n+3X/jferO9ytlw++3H+W+subvqwvb/qyinOLdcesO7Rq1irdPvN2jcse53Xsq7L9yHbVbavTj3b9\nSG1dbVowaYHW3rVWH533UeVl5HkdDwAASjT8qW8eurq82uMk/pKWkqZlpcu0rHSZvlb9NTW1NunZ\n957VM3uf0dPvPq0fvvZDpZgULZ66uH+WeuGUhUlxlrqts00/efMnqttWp1cPv6rstGx9dO5HtaZi\njSqmVDAPDgDwFUo0fMkJObpx0o0anzPe6yi+VpxXrI/P/7g+Pv/j6gn36NXDr2rDng165r1n9Lf1\nf6uv1H9F43PG646ZvWep75h1h+9+T9889qYeaXhEP3zthzrdcVo3TLhB/2fV/9H/eN//UEFWgdfx\nAAAYECUavtPe1a6tB7bqs4s/63WUpJKa0jsnvaRkib5a9VU1tzXr1+/9Whv2btCz7z2rx3c9LiOj\nRVMX9V+XetGURUpNSU141o7uDv18989Vt61OW/ZvUUZqhj58/Ye1pmKNlk1bxllnAIDvUaLhOy/s\nf0GdPZ3MQ1+lCbkT9LH3fUwfe9/HFLZhNRxu0Ia9G/TM3mf0tc1f01ef/6qKsot0+8zb+2epi/OK\nXc2058QerW1Yqx+89gMdbz+uWeNm6R9v+0fdf+P9vjtDDgDAUCjR8B0n6CgjNUPLS5d7HWXESDEp\nWjR1kRZNXaT/vfJ/60T7CT0XfK6/VP/4jR9LkhZOXth/lvrmkpvjsk12V0+XnnrnKdU11MkJOkpL\nSdM9196jNRVrVF1enRTz2gAAXIoSDd9xgo5umXaLcjNyvY4yYhXlFOneuffq3rn3KmzD2nl0Z//u\nid944Rt6aMtDKswq1G0zb+udpZ55hyaPiW0L1v2n9+u7Dd/Vozse1dHWoyrNL9VDVQ/pEws+EfPP\nAgDAbyjR8JXj7ce14+gOPVT1kNdRRo0Uk6KbJt+kmybfpL9e8dc6ee6knKDTf5b6iTefkCTdOOnG\n/it+LClZovTU9Mt+Vk+4R8/sfUZ1DXX67z3/LWut3n/N+7Vm4RrdOetOT+avAQBwAyUavrIxtFHS\nyNzqO1kUZhfqwzd8WB++4cOy1ur1pte1YW/vWep/+M0/6O9e+DvlZ+ardkZt/+jHiY4T+vrmr2vt\n9rXaf3q/JuVN0l8t/yv98U1/rOkF073+JQEAEHeUaPhKIBjQ2MyxqphS4XUUSDLGaP6k+Zo/ab6+\ntPxLOn3+tAKhQP/ox893/1ySlKIUhRVW7Yxa/fPt/6y7r717wDPVAACMFJRo+IoTclRVVhWXD7Qh\n/vKz8vXB6z6oD173QVlr9Wbzm9qwZ4N2vbtLf3P332h20WyvIwIAkBA0FfhG8GRQwZNBfWHJF7yO\ngigYYzR34lzNnThX9V31FGgAwKjCtaXgG4FgQJK4PjQAAPA9SjR8wwk5mjJmiq4tutbrKAAAAEOi\nRMMXwjasjaGNqp1Ry5bPAADA9yjR8IXXm17X8fbjqi1nlAMAAPgfJRq+4AQdSVwfGgAAJAdKNHzB\nCTq6fsL1mjJmitdRAAAArogSDc91dHdoc+NmRjkAAEDSoETDcy8efFHnus8xygEAAJIGJRqeCwQD\nSjWpWjl9pddRAAAAokKJhueckKPFUxcrPyvf6ygAAABRoUTDU6fPn9Yrh15hl0IAAJBUKNHwVP2+\neoVtmBINAACSCiUannKCjnLSc7SkZInXUQAAAKJGiYanAqGAVkxfoYzUDK+jAAAARI0SDc8cOnNI\nu4/v5vrQAAAg6VCi4ZlAKCBJzEMDAICkQ4mGZ5ygowk5EzSveJ7XUQAAAGJCiYYnrLVygo5qZtQo\nxfDHEAAAJBfaCzyx+/huHWk9oppytvoGAADJhxINTwSCzEMDAIDkRYmGJ5yQo5mFM1VWUOZ1FAAA\ngJi5VqKNMdcaY3Ze8HXGGPN5Y8w4Y8xzxpg9ke+FbmWAP3WHu7UptImz0AAAIGm5VqKtte9Ya2+0\n1t4oaaGkdkm/kPQlSQFr7WxJgch9jCKvHnpVZzvPUqIBAEDSStQ4R42k96y1jZLukbQu8vg6SR9I\nUAb4hBN0ZGRUVVbldRQAAIBhMdZa9w9izPckbbfWftsYc8paWxB53Eg62Xf/kveslrRakoqLixeu\nX7/e9ZyXam1tVV5eXsKPO9J9fufn1d7TrrUL1w7r/ayL/7Am/sS6+A9r4k+si/94tSZVVVUN1tqK\naF7reok2xmRIOizpBmtt04UlOvL8SWvtkHPRFRUVdtu2ba7mHEh9fb0qKysTftyRrK2zTYUPF+oL\nS76gh297eFg/g3XxH9bEn1gX/2FN/Il18R+v1sQYE3WJTsQ4xyr1noVuitxvMsZMlqTI92MJyACf\n2LJ/i7rCXcxDAwCApJaIEv1RST++4P5Tku6L3L5P0pMJyACfcIKOMlMztbx0uddRAAAAhs3VEm2M\nyZV0m6T/d8HDfy/pNmPMHkm1kfsYJZygo2Wly5Sdnu11FAAAgGFLc/OHW2vbJBVd8tgJ9V6tA6PM\nsbZjeq3pNX29+uteRwEAALgq7FiIhNkU2iSJrb4BAEDyo0QjYZygo/zMfC2cvNDrKAAAAFeFEo2E\nsNbqueBzqi6vVmpKqtdxAAAArgolGgkRPBlU4+lGRjkAAMCIQIlGQjhBR5JUU85nSgEAQPKjRCMh\nAqGASsaW6Jqia7yOAgAAcNUo0XBd2IYVCAVUO6NWxhiv4wAAAFw1SjRct/PoTrWca1FtOfPQAABg\nZKBEw3X989AzmIcGAAAjAyUarnOCjm6YcIMm5U3yOgoAAEBcUKLhqvPd57Vl/xYubQcAAEYUSjRc\n9eKBF3W++zwlGgAAjCiUaLjKCTpKNalaOX2l11EAAADihhINVzkhR0tKlmhM5hivowAAAMQNJRqu\nOXnupLYd3sYoBwAAGHEo0XBN/b56hW2Yrb4BAMCIQ4mGawKhgHLTc3Vzyc1eRwEAAIgrSjRc4wQd\nrSxbqYzUDK+jAAAAxBUlGq44cPqA3jnxDlt9AwCAEYkSDVcEQgFJ4kOFAABgRKJEwxVO0NHE3Ima\nO3Gu11EAAADijhKNuLPWKhAKqKa8RsYYr+MAAADEHSUacfdW81s62nqUUQ4AADBiUaIRd07QkcQ8\nNAAAGLko0Yg7J+Ro9rjZKs0v9ToKAACAKyjRiKuuni7V76vnLDQAABjRKNGIq1cOvaLWzla2+gYA\nACMaJRpxFQgFZGRUVV7ldRQAAADXUKIRV07Q0cIpCzUue5zXUQAAAFxDiUbctHa26sWDL7LVNwAA\nGPEo0YibzY2b1R3u5kOFAABgxKNEI26coKPM1EzdMu0Wr6MAAAC4ihKNuAmEAlpeulzZ6dleRwEA\nAHAVJRpx0dTapNebXmeUAwAAjAqUaMTFxtBGSWz1DQAARgdKNOLCCToqzCrUgkkLvI4CAADgOko0\nrpq1Vs8Fn1N1ebVSU1K9jgMAAOA6SjSu2t6WvTpw5gBbfQMAgFGDEo2rFggFJDEPDQAARg9KNK6a\nE3RUml+qWeNmeR0FAAAgISjRuCo94R5tDG1UbXmtjDFexwEAAEgISjSuyo6jO3Ty/ElGOQAAwKhC\nicZVcYKOJKm6vNrjJAAAAIlDicZVCYQCmjdxnorzir2OAgAAkDCulmhjTIEx5mfGmLeNMbuNMUuN\nMeOMMc8ZY/ZEvhe6mQHuOdd1TlsatzDKAQAARh23z0R/S9Iz1to5kuZL2i3pS5IC1trZkgKR+0hC\nWw9sVUdPByUaAACMOq6VaGNMvqQVkh6TJGttp7X2lKR7JK2LvGydpA+4lQHucoKO0lLStGL6Cq+j\nAAAAJJSx1rrzg425UdJaSW+p9yx0g6TPSTpkrS2IvMZIOtl3/5L3r5a0WpKKi4sXrl+/3pWcQ2lt\nbVVeXl7Cj5ss1mxfowyToX9b8G8JPS7r4j+siT+xLv7DmvgT6+I/Xq1JVVVVg7W2IprXprmYI03S\nTZI+Y6192RjzLV0yumGttcaYAVu8tXateku4KioqbGVlpYtRB1ZfXy8vjpsMWs616N3n39VXVn4l\n4b9HrIv/sCb+xLr4D2viT6yL/yTDmrg5E31Q0kFr7cuR+z9Tb6luMsZMlqTI92MuZoBL6vfVy8oy\nDw0AAEYl10q0tfaopAPGmGsjD9Wod7TjKUn3RR67T9KTbmWAe5ygo7yMPC2eutjrKAAAAAnn5jiH\nJH1G0uPGmAxJQUl/pN7i/oQx5gFJjZI+4nIGuMAJOqosq1R6arrXUQAAABLO1RJtrd0paaDh7Bo3\njwt3NZ5q1J6WPfrUok95HQUAAMAT7FiImAVCAUlSzQz+LgQAAEYnSjRiFggFVJxbrBsm3OB1FAAA\nAE9QohETa62coKPaGbXqvcw3AADA6EOJRkzeOPaGjrUd49J2AABgVKNEIyZO0JEk1ZQzDw0AAEYv\nSjRi4oQcXVN0jablT/M6CgAAgGco0YhaV0+Xnt/3vGrLGeUAAACjGyUaUXv50Mtq62pjHhoAAIx6\nlGhEzQk6SjEpqiyr9DoKAACApyjRiJoTdFQxpUKF2YVeRwEAAPAUJRpROdNxRi8dfIl5aAAAAFGi\nEaXNjZvVY3vY6hsAAECUaEQpEAwoKy1Lt0y7xesoAAAAnqNEIypOyNGtpbcqKy3L6ygAAACeo0Tj\nio62HtWqLVwSAAAgAElEQVQbx97g0nYAAAARlGhcUSAYkCRKNAAAQAQlGlfkhByNyx6nGyfd6HUU\nAAAAX6BEY0jWWgWCAVWXVyvF8McFAABAokTjCva07NGBMwe4PjQAAMAFKNEYkhN0JDEPDQAAcCFK\nNIbkBB2VFZRpRuEMr6MAAAD4BiUag+oJ92hjaKNqy2tljPE6DgAAgG9QojGohiMNOt1xmq2+AQAA\nLkGJxqD6rg9dXV7tcRIAAAB/oURjUE7I0fzi+ZqYO9HrKAAAAL5CicaA2rva9cL+F7gqBwAAwAAo\n0RjQb/b/Rp09nZRoAACAAVCiMSAn6Cg9JV23lt7qdRQAAADfoURjQIFQQEunLVVuRq7XUQAAAHyH\nEo3LnGg/oe1HtrPVNwAAwCAo0bjMpn2bZGWZhwYAABgEJRqXcYKOxmSM0aKpi7yOAgAA4EuUaFzG\nCTqqKq9SWkqa11EAAAB8iRKNi+w7tU/vnXxPNeVs9Q0AADAYSjQu0rfVN/PQAAAAg6NE4yJOyNHk\nvMm6bvx1XkcBAADwLUo0+oVtWIFgQLUzamWM8ToOAACAb1Gi0W9X0y41tzczygEAAHAFlGj0c4KO\nJPGhQgAAgCugRKNfIBTQnPFzNHXsVK+jAAAA+BolGpKkzp5OPd/4PFt9AwAARIESDUnSSwdfUntX\nO/PQAAAAUaBEQ1LvPHSKSVFlWaXXUQAAAHzP1X2djTH7JJ2V1COp21pbYYwZJ+knksok7ZP0EWvt\nSTdz4MqcoKNFUxYpPyvf6ygAAAC+l4gz0VXW2huttRWR+1+SFLDWzpYUiNyHh850nNErh15hlAMA\nACBKXoxz3CNpXeT2Okkf8CADLvD8vufVY3so0QAAAFFyu0RbSb82xjQYY1ZHHiu21h6J3D4qqdjl\nDLgCJ+goOy1bS0uWeh0FAAAgKRhrrXs/3Jip1tpDxpiJkp6T9BlJT1lrCy54zUlrbeEA710tabUk\nFRcXL1y/fr1rOQfT2tqqvLy8hB830e5/9X5NzJyof3jfP3gdJSqjZV2SCWviT6yL/7Am/sS6+I9X\na1JVVdVwwQjykFz9YKG19lDk+zFjzC8kLZbUZIyZbK09YoyZLOnYIO9dK2mtJFVUVNjKyko3ow6o\nvr5eXhw3kQ6fPazG5xv16WWfVuUtlV7HicpoWJdkw5r4E+viP6yJP7Eu/pMMa+LaOIcxJtcYM6bv\ntqTbJb0h6SlJ90Vedp+kJ93KgCsLBAOS2OobAAAgFm6eiS6W9AtjTN9xfmStfcYY86qkJ4wxD0hq\nlPQRFzPgCgKhgIqyizR/0nyvowAAACSNK5ZoY8xnJP1nrNdyttYGJV3WzKy1JyRx2tMHrLVygo5q\nZtQoxbDvDgAAQLSiaU7Fkl41xjxhjLnTRE4tI/m9c+IdHTp7SLXlXNoOAAAgFlcs0dbaL0uaLekx\nSfdL2mOM+YYxZqbL2eAyJ+hIEteHBgAAiFFU/4Zve6+DdzTy1S2pUNLPjDHJcU00DMgJOiovKFd5\nYbnXUQAAAJLKFUu0MeZzxpgGSf8g6TeS5llr/0TSQkm/73I+uKQ73K36ffWchQYAABiGaK7OMU7S\nB621jRc+aK0NG2PucicW3NZwuEGnO05TogEAAIYhmnGODZJa+u4YY8YaY26WJGvtbreCwV1989DV\n5dUeJwEAAEg+0ZTo70hqveB+a+QxJDEn5GjBpAUanzPe6ygAAABJJ5oSbSIfLJTUO8Yhl7cLh7va\nOtu09cBWRjkAAACGKZoSHTTGfNYYkx75+pykoNvB4J4X9r+gzp5OtvoGAAAYpmhK9BpJt0g6JOmg\npJslrXYzFNwVCAWUkZqh5aXLvY4CAACQlK44lmGtPSbp3gRkQYI4QUe3TLtFuRm5XkcBAABISlcs\n0caYLEkPSLpBUlbf49baT7iYCy453n5cO47u0ENVD3kdBQAAIGlFM87xH5ImSbpD0vOSSiSddTMU\n3LMxtFESW30DAABcjWhK9Cxr7d9IarPWrpP0fvXORSMJOUFHYzPHauGUhV5HAQAASFrRlOiuyPdT\nxpi5kvIlTXQvEtwUCAVUVValtBSuUggAADBc0ZTotcaYQklflvSUpLckPexqKrgieDKo4MkgoxwA\nAABXacjTkcaYFElnrLUnJW2WNCMhqeCKQDAgiXloAACAqzXkmejI7oR/nqAscJkTcjR1zFRdW3St\n11EAAACSWjTjHI4x5ovGmGnGmHF9X64nQ1yFbViBYEC1M2pljPE6DgAAQFKL5tNlfxD5/qkLHrNi\ntCOpvHb0NZ04d4KtvgEAAOIgmh0LyxMRBO4KhHrnoWtmUKIBAACuVjQ7Fn58oMettT+Mfxy4xQk6\nun7C9ZoyZorXUQAAAJJeNOMciy64nSWpRtJ2SZToJNHR3aHNjZv1yZs+6XUUAACAESGacY7PXHjf\nGFMgab1riRB3Lx58Uee6z3FpOwAAgDiJ5uocl2qTxJx0EnGCjlJNqlaWrfQ6CgAAwIgQzUz0r9R7\nNQ6pt3RfL+kJN0MhvgKhgBZPXayxmWO9jgIAADAiRDMT/c0LbndLarTWHnQpD+Ls9PnTeuXQK/rr\nW//a6ygAAAAjRjQler+kI9ba85JkjMk2xpRZa/e5mgxxUb+vXmEbZh4aAAAgjqKZif6ppPAF93si\njyEJOEFHOek5WlKyxOsoAAAAI0Y0JTrNWtvZdydyO8O9SIgnJ+Ro5fSVykhlyQAAAOIlmhLdbIy5\nu++OMeYeScfdi4R4OXTmkN4+/jZbfQMAAMRZNDPRayQ9boz5duT+QUkD7mIIf+nb6pt5aAAAgPiK\nZrOV9yQtMcbkRe63up4KceEEHU3ImaB5xfO8jgIAADCiXHGcwxjzDWNMgbW21VrbaowpNMY8lIhw\nGD5rrZygo5oZNUoxw9lTBwAAAIOJpl2tstae6rtjrT0p6Xfci4R42H18t460HlFtOaMcAAAA8RZN\niU41xmT23THGZEvKHOL18AEn6EiSambwoUIAAIB4i+aDhY9LChhjvi/JSLpf0jo3Q+HqBUIBzSyc\nqbKCMq+jAAAAjDjRfLDwYWPMa5JqJVlJz0qa7nYwDF93uFubQpv0h/P+0OsoAAAAI1K0nzhrUm+B\n/rCkakm7XUuEq/bqoVd1tvMsl7YDAABwyaBnoo0x10j6aOTruKSfSDLW2qoEZcMwOUFHRkZVZSwV\nAACAG4Ya53hb0hZJd1lr90qSMeYLCUmFq+KEHC2YvEBFOUVeRwEAABiRhhrn+KCkI5I2GWO+a4yp\nUe8HC+FjbZ1tevHAi1zaDgAAwEWDlmhr7S+ttfdKmiNpk6TPS5pojPmOMeb2RAVEbLbs36KucBfz\n0AAAAC664gcLrbVt1tofWWt/V1KJpB2S/sL1ZBgWJ+goMzVTy0uXex0FAABgxIppP2hr7Ulr7Vpr\nbdQ7eBhjUo0xO4wxT0fulxtjXjbG7DXG/MQYkxFraAzOCTpaVrpM2enZXkcBAAAYsWIq0cP0OV18\nSbyHJf2LtXaWpJOSHkhAhlHhWNsxvdb0GvPQAAAALnO1RBtjSiS9X9KjkftGvdeZ/lnkJeskfcDN\nDKPJxtBGSWz1DQAA4DZjrXXvhxvzM0l/J2mMpC+qd8vwlyJnoWWMmSZpg7V27gDvXS1ptSQVFxcv\nXL9+vWs5B9Pa2qq8vLyEH3e4vvnON1XfXK8nlz2pVJPqdRzXJNu6jAasiT+xLv7DmvgT6+I/Xq1J\nVVVVg7W2IprXXnHb7+Eyxtwl6Zi1tsEYUxnr+621ayWtlaSKigpbWRnzj7hq9fX18uK4w2Gt1f07\n79fts29XTdXIPhOdTOsyWrAm/sS6+A9r4k+si/8kw5q4Oc6xTNLdxph9ktard4zjW5IKjDF95b1E\n0iEXM4wawZNBNZ5u5NJ2AAAACeBaibbW/qW1tsRaWybpXkkbrbUfU+81pz8Uedl9kp50K8No4gQd\nSaJEAwAAJEAirs5xqb+Q9KfGmL2SiiQ95kGGEccJOSoZW6LZ42Z7HQUAAGDEc20m+kLW2npJ9ZHb\nQUmLE3Hc0SJsw9oY2qi7r71bvRdAAQAAgJu8OBONONt5dKdazrVwfWgAAIAEoUSPAH3z0FwfGgAA\nIDEo0SOAE3Q0d+JcTcqb5HUUAACAUYESneTOd5/Xlv1bGOUAAABIIEp0ktt6YKvOd59nlAMAACCB\nKNFJLhAMKNWkauX0lV5HAQAAGDUo0UnOCTlaUrJEYzLHeB0FAABg1KBEJ7GT505q2+Ft7FIIAACQ\nYJToJFa/r15hG6ZEAwAAJBglOok5QUe56blaPJUNIAEAABKJEp3EAqGAVpatVEZqhtdRAAAARhVK\ndJI6cPqA3jnxDteHBgAA8AAlOkkFQgFJYh4aAADAA5ToJOUEHU3Mnai5E+d6HQUAAGDUoUQnIWut\nnKCj2hm1MsZ4HQcAAGDUoUQnoTeb31RTW5NqytnqGwAAwAuU6CQUCDIPDQAA4CVKdBJyQo5mj5ut\n0vxSr6MAAACMSpToJNPV06X6ffWchQYAAPAQJTrJvHLoFbV2tlKiAQAAPESJTjJO0JGRUWVZpddR\nAAAARi1KdJIJhAJaOGWhxmWP8zoKAADAqEWJTiKtna168eCLbPUNAADgMUp0EtncuFnd4W7moQEA\nADxGiU4iTtBRVlqWlpUu8zoKAADAqEaJTiJO0NGyacuUlZbldRQAAIBRjRKdJJpam7Tr2C5GOQAA\nAHyAEp0kNoY2SmKrbwAAAD+gRCcJJ+ioMKtQCyYt8DoKAADAqEeJTgLWWj0XfE7V5dVKTUn1Og4A\nAMCoR4lOAntb9urAmQOMcgAAAPgEJToJOEFHklRTXuNxEgAAAEiU6KQQCAVUml+qWeNmeR0FAAAA\nokT7Xk+4RxtDG1VbXitjjNdxAAAAIEq07+04ukMnz59kHhoAAMBHKNE+1zcPXV1e7XESAAAA9KFE\n+5wTdDRv4jwV5xV7HQUAAAARlGgfO9d1Ti/sf4FRDgAAAJ+hRPvY1gNb1dHTQYkGAADwGUq0jzlB\nR2kpaVoxfYXXUQAAAHABSrSPOSFHS0uWKi8jz+soAAAAuAAl2qdazrWo4XADoxwAAAA+RIn2qU2h\nTbKybPUNAADgQ5RonwqEAsrLyNPiqYu9jgIAAIBLuFaijTFZxphXjDGvGWPeNMZ8NfJ4uTHmZWPM\nXmPMT4wxGW5lSGZO0FFlWaXSU9O9jgIAAIBLuHkmukNStbV2vqQbJd1pjFki6WFJ/2KtnSXppKQH\nXMyQlBpPNWpPyx7VljMPDQAA4EeulWjbqzVyNz3yZSVVS/pZ5PF1kj7gVoZkFQgFJIkPFQIAAPiU\nsda698ONSZXUIGmWpH+X9I+SXoqchZYxZpqkDdbauQO8d7Wk1ZJUXFy8cP369a7lHExra6vy8hJ/\nebmv7f6adp7aqZ8t+ZmMMQk/vt95tS4YHGviT6yL/7Am/sS6+I9Xa1JVVdVgra2I5rVpbgax1vZI\nutEYUyDpF5LmxPDetZLWSlJFRYWtrKx0JeNQ6uvrlejjWmv1B9v+QKuuXaWqqqqEHjtZeLEuGBpr\n4k+si/+wJv7EuvhPMqxJQq7OYa09JWmTpKWSCowxfeW9RNKhRGRIFm8ce0PH2o4xygEAAOBjbl6d\nY0LkDLSMMdmSbpO0W71l+kORl90n6Um3MiQjJ+hIEteHBgAA8DE3xzkmS1oXmYtOkfSEtfZpY8xb\nktYbYx6StEPSYy5mSDpOyNG1RddqWv40r6MAAABgEK6VaGvt65IWDPB4UBI7iAygs6dTz+97Xvff\neL/XUQAAADAEdiz0kZcPvqy2rjZGOQAAAHyOEu0jgVBAKSZFlWWVXkcBAADAECjRPuIEHVVMqVBh\ndqHXUQAAADAESrRPnOk4o5cOvsRW3wAAAEmAEu0Tmxs3q8f2cH1oAACAJECJ9gkn6CgrLUtLpy31\nOgoAAACugBLtE4FQQLeW3qqstCyvowAAAOAKKNE+cLT1qN449gajHAAAAEmCEu0DgWBAkijRAAAA\nSYIS7QNOyNG47HG6cdKNXkcBAABAFCjRHrPWygk6qimvUYphOQAAAJIBrc1je1r26OCZg2z1DQAA\nkEQo0R5zgo4k5qEBAACSCSXaY07QUVlBmWYUzvA6CgAAAKJEifZQT7hHG0MbVVteK2OM13EAAAAQ\nJUq0hxqONOh0x2lGOQAAAJIMJdpDffPQ1eXVHicBAABALCjRHgqEAppfPF8Tcid4HQUAAAAxoER7\npL2rXS/sf4FRDgAAgCREifbIb/b/Rp09nZRoAACAJESJ9ogTdJSekq5bS2/1OgoAAABiRIn2iBNy\ntHTaUuVm5HodBQAAADGiRHvgRPsJ7TiyQ7XljHIAAAAkI0q0Bzbt2yQryzw0AABAkqJEe8AJOhqT\nMUaLpi7yOgoAAACGgRLtASfoqKq8SmkpaV5HAQAAwDBQohMsdDKk906+xzw0AABAEqNEJ1ggFJAk\n1cyo8TgJAAAAhosSnWCBUECT8ybruvHXeR0FAAAAw0SJTqCwDSsQDKh2Rq2MMV7HAQAAwDBRohNo\nV9MuNbc3c2k7AACAJEeJTiAn6EiSasqZhwYAAEhmlOgEckKO5oyfo6ljp3odBQAAAFeBEp0gnT2d\n2ty4mUvbAQAAjACU6AR56eBLau9qZx4aAABgBKBEJ4gTdJRiUlRZVul1FAAAAFwlSnSCOEFHi6cu\nVn5WvtdRAAAAcJUo0Qlw+vxpvXLoFeahAQAARghKdAI83/i8emwPW30DAACMEJToBAgEA8pOy9bS\nkqVeRwEAAEAcUKITwAk5WjF9hTLTMr2OAgAAgDigRLvs8NnDeqv5LS5tBwAAMIJQol0WCAYkiRIN\nAAAwglCiXeaEHI3PGa/3Fb/P6ygAAACIE9dKtDFmmjFmkzHmLWPMm8aYz0UeH2eMec4YsyfyvdCt\nDF6z1ioQDKi6vFophr+vAAAAjBRuNrtuSX9mrb1e0hJJnzLGXC/pS5IC1trZkgKR+yPSOyfe0aGz\nh7g+NAAAwAjjWom21h6x1m6P3D4rabekqZLukbQu8rJ1kj7gVgavOUFHEvPQAAAAI42x1rp/EGPK\nJG2WNFfSfmttQeRxI+lk3/1L3rNa0mpJKi4uXrh+/XrXc16qtbVVeXl5w37/l9/4skJtIT1+8+Nx\nTIWrXRfEH2viT6yL/7Am/sS6+I9Xa1JVVdVgra2I5rVpbocxxuRJ+rmkz1trz/T25l7WWmuMGbDF\nW2vXSlorSRUVFbaystLtqJepr6/XcI/bHe7Wrpd26d4b7h32z8DArmZd4A7WxJ9YF/9hTfyJdfGf\nZFgTVz/tZoxJV2+Bftxa+/8iDzcZYyZHnp8s6ZibGbzScLhBZzrOsNU3AADACOTm1TmMpMck7bbW\n/vMFTz0l6b7I7fskPelWBi/1zUNXl1d7nAQAAADx5uY4xzJJ/1PSLmPMzshjfyXp7yU9YYx5QFKj\npI+4mMEzTsjRgkkLND5nvNdRAAAAEGeulWhr7QuSzCBPj+gZh7bONm09sFWfu/lzXkcBAACAC9gB\nxAUv7H9BnT2dXNoOAABghKJEu8AJOspIzdDy0uVeRwEAAIALKNEuCIQCumXaLcpJz/E6CgAAAFxA\niY6z4+3HtePoDrb6BgAAGMEo0XG2MbRRElt9AwAAjGSU6Dhzgo7yM/O1cMpCr6MAAADAJZToOHOC\njqrKq5SW4vqO6gAAAPAIJTqOgieDCp0KqaZ8RF8GGwAAYNSjRMdRIBiQxDw0AADASEeJjiMn5Gjq\nmKm6tuhar6MAAADARZToOAnbsALBgGpn1MqYwXY7BwAAwEhAiY6T146+phPnTjDKAQAAMApQouPE\nCTqSpOryao+TAAAAwG2U6DgJhAK6fsL1mjJmitdRAAAA4DJKdBx0dHdoc+NmtvoGAAAYJSjRcfDi\nwRd1rvsc89AAAACjBCU6Dpygo1STqpVlK72OAgAAgASgRMeBE3S0eOpijc0c63UUAAAAJAAl+iqd\nPn9arx5+lVEOAACAUYQSfZXq99UrbMOUaAAAgFGEEn2VnKCjnPQcLSlZ4nUUAAAAJAgl+io5IUcr\np69URmqG11EAAACQIJToq3DwzEG9ffxtRjkAAABGGUr0VQgEA5KkmvIaj5MAAAAgkSjRVyEQCmhC\nzgTNK57ndRQAAAAkECV6mKy1coKOambUKMXw2wgAADCa0P6Gaffx3TrSekS15cxDAwAAjDaU6GFy\ngo4k8aFCAACAUYgSPUxO0NHMwpmaXjDd6ygAAABIMEr0MHSHu1W/r56z0AAAAKMUJXoYXj30qs52\nnqVEAwAAjFKU6GFwgo6MjKrKqryOAgAAAA9QoofBCTm6afJNKsop8joKAAAAPECJjlFrZ6tePPAi\noxwAAACjGCU6Rlsat6gr3MVW3wAAAKMYJTpGgVBAmamZWl663OsoAAAA8AglOkZO0NGy0mXKTs/2\nOgoAAAA8QomOwbG2Y3qt6TW2+gYAABjlKNEx2BjaKImtvgEAAEY7SnQMnKCjgqwC3TT5Jq+jAAAA\nwEOU6ChZa+UEHVWVVSk1JdXrOAAAAPAQJTpKwZNBNZ5uZJQDAAAAlOhoOUFHEvPQAAAAcLFEG2O+\nZ4w5Zox544LHxhljnjPG7Il8L3Tr+PHmhBxNGztNs8fN9joKAAAAPObmmegfSLrzkse+JClgrZ0t\nKRC573thG9bG0EbVzqiVMcbrOAAAAPCYayXaWrtZUsslD98jaV3k9jpJH3Dr+PG08+hOtZxrYatv\nAAAASEr8THSxtfZI5PZRScUJPv6w9M1D18ygRAMAAEAy1lr3frgxZZKettbOjdw/Za0tuOD5k9ba\nAeeijTGrJa2WpOLi4oXr1693LedgWltblZeXp//1+v/Sic4T+l7F9xKeAZfrWxf4B2viT6yL/7Am\n/sS6+I9Xa1JVVdVgra2I5rVpboe5RJMxZrK19ogxZrKkY4O90Fq7VtJaSaqoqLCVlZUJivhb9fX1\nWrJ8id74zRtas3CNvMiAy9XX17MWPsOa+BPr4j+siT+xLv6TDGuS6HGOpyTdF7l9n6QnE3z8mG09\nsFXnu89zaTsAAAD0c/MSdz+W9KKka40xB40xD0j6e0m3GWP2SKqN3Pc1J+goLSVNK6av8DoKAAAA\nfMK1cQ5r7UcHeSqpPp0XCAV089SbNSZzjNdRAAAA4BPsWDiEs11nte3wNkY5AAAAcBFK9BB2nt6p\nsA1TogEAAHARSvQQGk42KC8jTzdPvdnrKAAAAPARSvQQtp/crhXTVyg9Nd3rKAAAAPARSvQgDpw+\noAPnDqi2nFEOAAAAXIwSPYhAKCBJzEMDAADgMpToQfSEezRnzBzNnTjX6ygAAADwmURv+500Hrjp\nAc08M1PGGK+jAAAAwGc4Ew0AAADEiBINAAAAxIgSDQAAAMSIEg0AAADEiBINAAAAxIgSDQAAAMSI\nEg0AAADEiBINAAAAxIgSDQAAAMSIEg0AAADEiBINAAAAxIgSDQAAAMSIEg0AAADEiBINAAAAxIgS\nDQAAAMSIEg0AAADEiBINAAAAxIgSDQAAAMTIWGu9znBFxphmSY0eHHq8pOMeHBdDY138hzXxJ9bF\nf1gTf2Jd/MerNZlurZ0QzQuTokR7xRizzVpb4XUOXIx18R/WxJ9YF/9hTfyJdfGfZFgTxjkAAACA\nGFGiAQAAgBhRooe29v+3d78hd9Z1HMffHzdXus0WGLJ2DxT8A2Gga1hjNreyYSjpg8AGRfXEJyaK\nD8RKkJ76QEUjn2wLI5uacyAiS6GJf3L+2ZquOR1DJt5TmRKV68FE/frgXLLjdk+8kPqd43m/4Ob6\nc5/r8Dn8uLm//M73+l2tA2hGjsvocUxGk+MyehyT0eS4jJ6RHxN7oiVJkqSenImWJEmSerKIliRJ\nknqyiD6GJBcleTnJ3iTXt84jSLI+yYEk/2idRQNJFifZkuTFJLuSXN0606RL8sUkzyR5vhuT37TO\npIEks5L8PcmDrbNoIMm+JDuT7EjyXOs8GkiyIMl9SV5KsjvJstaZZmJP9AySzAL2AN8DpoFngTVV\n9WLTYBMuyQrgIPCHqjq7dR5BkoXAwqranmQ+sA24zL+VdpIEmFtVB5McDzwBXF1VWxtHm3hJrgWW\nAidV1SWt82hQRANLq8oHrYyQJHcCj1fV2iRzgBOr6l+tcx3JmeiZnQfsrapXqupd4G7g0saZJl5V\nPQb8s3UOHVZVb1TV9m7/HWA3sKhtqslWAwe7w+O7H2dLGksyBVwMrG2dRRplSb4ErADWAVTVu6NY\nQINF9LEsAl4bOp7GwkD6RElOBc4Fnm6bRF3bwA7gAPBIVTkm7d0KXAd80DqIPqaAh5NsS3JF6zAC\n4DTgLeD3XfvT2iRzW4eaiUW0pM8syTxgI3BNVf2ndZ5JV1XvV9U5wBRwXhLbnxpKcglwoKq2tc6i\no5xfVUuA7wNXdm2Dams2sAS4o6rOBf4LjOS9aRbRM9sPLB46nurOSTpC13e7Ebirqu5vnUeHdV+B\nbgEuap1lwi0HftD1394NfCfJH9tGEkBV7e+2B4BNDNo51dY0MD30Ddp9DIrqkWMRPbNngTOSnNY1\ntP8IeKBxJmnkdDexrQN2V9XNrfMIknwlyYJu/wQGN0i/1DbVZKuqX1bVVFWdyuD/yV+r6seNY028\nJHO7G6Lp2gVWA67+1FhVvQm8luSs7tR3gZG8WX126wCjqKreS/IL4C/ALGB9Ve1qHGviJdkArARO\nTjIN3FhV69qmmnjLgZ8AO7seXIBfVdVDDTNNuoXAnd0qQ8cB91aVS6pJRzsF2DSYC2A28Keq2tw2\nkjpXAXd1E5mvAD9vnGdGLnEnSZIk9WQ7hyRJktSTRbQkSZLUk0W0JEmS1JNFtCRJktSTRbQkSZLU\nk0W0JI2ZJL9OsivJC0l2JPlmkmuSnNg6myRNCpe4k6QxkmQZcDOwsqoOJTkZmAP8DVhaVW83DShJ\nEy+v6KsAAAFfSURBVMKZaEkaLwuBt6vqEEBXNP8Q+CqwJckWgCSrkzyVZHuSPyeZ153fl+SmJDuT\nPJPk9FYfRJLGmUW0JI2Xh4HFSfYk+V2SC6rqNuB1YFVVrepmp28ALqyqJcBzwLVD7/Hvqvo68Fvg\n1v/3B5CkzwMf+y1JY6SqDib5BvBtYBVwT5Lrj3jZt4CvAU92jzSeAzw19PsNQ9tb/reJJenzySJa\nksZMVb0PPAo8mmQn8NMjXhLgkapac6y3OMa+JOlTsp1DksZIkrOSnDF06hzgVeAdYH53biuw/KN+\n5yRzk5w5dM3lQ9vhGWpJ0qfkTLQkjZd5wO1JFgDvAXuBK4A1wOYkr3d90T8DNiT5QnfdDcCebv/L\nSV4ADnXXSZJ6cok7SZogSfbhUniS9JnZziFJkiT15Ey0JEmS1JMz0ZIkSVJPFtGSJElSTxbRkiRJ\nUk8W0ZIkSVJPFtGSJElSTx8CzByywZiyylwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb6850a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(acc, color='g')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Network L2 regularization\n",
    "In L2 regularization there is a new regularization parameter. The right amount of regularization should improve your validation / test accuracy. Let's see what the best value for that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Test Regul./Accuracy: '1.000e-04' 86.46\n",
      "Test Regul./Accuracy: '1.259e-04' 86.09\n",
      "Test Regul./Accuracy: '1.585e-04' 86.67\n",
      "Test Regul./Accuracy: '1.995e-04' 87.02\n",
      "Test Regul./Accuracy: '2.512e-04' 87.25\n",
      "Test Regul./Accuracy: '3.162e-04' 87.41\n",
      "Test Regul./Accuracy: '3.981e-04' 87.91\n",
      "Test Regul./Accuracy: '5.012e-04' 87.97\n",
      "Test Regul./Accuracy: '6.310e-04' 88.44\n",
      "Test Regul./Accuracy: '7.943e-04' 88.86\n",
      "Test Regul./Accuracy: '1.000e-03' 88.87\n",
      "Test Regul./Accuracy: '1.259e-03' 89.07\n",
      "Test Regul./Accuracy: '1.585e-03' 89.12\n",
      "Test Regul./Accuracy: '1.995e-03' 89.09\n",
      "Test Regul./Accuracy: '2.512e-03' 89.15\n",
      "Test Regul./Accuracy: '3.162e-03' 89.1\n",
      "Test Regul./Accuracy: '3.981e-03' 89.05\n",
      "Test Regul./Accuracy: '5.012e-03' 89.03\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "print(\"Initialized\")\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    print(\"Test Regul./Accuracy: '%.3e'\" % regul, accuracy_val[-1])\n",
    "print(\"Ended\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting regularization best value parameter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularitazion')\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### L2 regularization\n",
    "\n",
    "The idea is to add another term to the loss, which penalizes large weights.\n",
    "It's typically achieved by adding the L2 norm of your weights to the loss, multiplied by a small constant.\n",
    "\n",
    "Let's try now adding this new parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      losses.append(l)\n",
    "      acc.append(accuracy(predictions, batch_labels))\n",
    "      valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['L2 Reg. model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plotting loss and accuracy by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(acc, color='g')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "\n",
    "print(\"Initialized\")\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))  \n",
    "    \n",
    "    print(\"Test Regul./Accuracy: '%.3e'\" % regul, accuracy_val[-1])\n",
    "print(\"Ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the L2 parameter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Problem 1\n",
    "\n",
    "We've got improved Test Accuracy from 89.1% with Logistic Regressioon to 93.2% with a 1-layer Neural Network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Overfitting model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion Problem 2\n",
    "\n",
    "The generalization capability is poor, as shown in the validation and test accuracy. Since there are far too much parameters and no regularization, the accuracy of the batches is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Dropout model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion Problem 3\n",
    "\n",
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['MultiLayer 2L model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Multilayer 3L model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  methodDict['Multilayer 4L model'] = accuracy(\n",
    "        valid_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSuccess():\n",
    "    s = pd.Series(methodDict)\n",
    "    \n",
    "    #Colors\n",
    "    ax = s.plot(kind='bar', figsize=(12, 6))\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    plt.ylim([70.0, 100.0])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Success of methods')\n",
    "     \n",
    "    plt.show()\n",
    "plotSuccess()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
