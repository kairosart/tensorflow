{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Validation set perplexity\n",
    "\n",
    "We'll plot a graph with the Validation set perplexity results of each model at the end of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methodDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "### Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-f6040ce17d4a>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300495 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "pzkhnccg wi onyh l n odchrlnkmgy aifidakltjxasd s ettffowwncybzkjrtvil gbu hcbnu\n",
      "rnr yhrhn twoi apq q hjgnflfyo rmqo mzblfwigvgherfxjtpw nbi oajaz ocgqofypu yqof\n",
      "euna i iadiuhix wvti yuycu rskr tpamt eqdhzr jksxgaxjiijealltnzs iplopi hd e t a\n",
      "tcnmz phccem   kyfx novplrufcehwao fycisdsezeuoph n nu m tee xlswzsey  adi miuq \n",
      "vi ebipnpqsz unt otunniwz csgeo ih   remguzwx nfji ijbzuklqpprlfd ivdmzee  irita\n",
      "================================================================================\n",
      "Validation set perplexity: 20.25\n",
      "Average loss at step 100: 2.610598 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.12\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 200: 2.252762 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 300: 2.097437 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 400: 2.001021 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 500: 1.936431 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 600: 1.908252 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 700: 1.859561 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 800: 1.818377 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 900: 1.827503 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.830374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "paille grould sich the  or conzers problects of the cencles technoum whe no ono \n",
      "presies existor kkes of the zeroled of and sunce of doxal in the ity from kah u \n",
      "y from to thee ias a mos unives of the one gen surd axace resolame rond of defel\n",
      "th fambe syamed house sich is of then grot nable in newifomentrear weop apoete n\n",
      "xituid comperion of pan as one two cred jualm an the asties is state beang misto\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1100: 1.776508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.757351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.736722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.749014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.739802 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1600: 1.750888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1700: 1.716455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.678043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.653412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.702247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "pieshion pant list ishistalling fundial windding decansual appogttisy use eight \n",
      "onds lape and acury aumpated s in imendence to imour and subussiy inlynel but wo\n",
      "forced is one nine seven starnion many by in to crawnied rovect wot of a ammitsa\n",
      "lism state zero vingd hownon of not mordignane poputersing formonc ve grovings w\n",
      "ly and tet reconspantimanachonive commains loting that shoog one six six priside\n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2100: 1.690470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2200: 1.687956 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.645944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.663997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2500: 1.684299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2600: 1.659763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.660057 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.653755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.654517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3000: 1.655558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "side computer mucional fistoriagrens two zero six three har infurtersqrices him \n",
      "rectly refer eight to shis the simbing dice poorataric transh which official per\n",
      "z some of becomize the sal l scallingotor whable other indiant as aginvigation k\n",
      "y in wotlon sponed and after are of the drifigst of served land mea pardian user\n",
      "dubakir words eighere son kawar senal moevend hanaphiction oth catscriber caw al\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3100: 1.632888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3200: 1.650224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.640961 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.672224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3500: 1.659865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.666817 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3700: 1.649023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3800: 1.646577 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3900: 1.637840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.655519 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "gains there age ecoged in became brom more indicated aboc frati is neber theo wh\n",
      "her poestelly the cults of monthous crevelf eavily to theory the he homspy us is\n",
      "husissing and goern dorsos the uglumy y one rund begick imperially fitals asanes\n",
      "fores antiben instics with the keopred first the brysimines incequeption of hope\n",
      "neters entries and kight eppre provect laokes of withur serticizent emorthes age\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100: 1.638358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.638440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.617160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.611263 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.615870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.614821 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.629054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.632711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4900: 1.634461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5000: 1.609699 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "ab xoess the amin s in the im same up out often liberies confinual quanity over \n",
      "noter jud in metamines or twife berors norked wall bairns can from sosure aberit\n",
      "x living to beccatesos vone examplases by some artiless peoserta musil western o\n",
      "etion ifs one nine five eighelorgeely often they di a rypratte as limz frops use\n",
      "vely forms of romarty one nine five how t gnete payer god about a winhdon as ofs\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.609514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5200: 1.598587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5300: 1.585913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5400: 1.578138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5500: 1.568343 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5600: 1.583397 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5700: 1.573606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5800: 1.586805 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5900: 1.578354 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6000: 1.551114 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "vey one nincon low c notefore of the counce france stand haris gugen of has schu\n",
      "ked about the pill of leapy to gation arosgreas musess united depass there basin\n",
      "pers his one five nine one nine with internation mark chaisonee ay generally in \n",
      "in toolity that it iffaction of hoirds muried aspaint hark on jurudent in fislin\n",
      "zal differentared if smethman driu spone to vyl and holiains applese one nine ni\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6100: 1.564999 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6200: 1.537271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6300: 1.549587 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6400: 1.546603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6500: 1.561923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 6600: 1.596258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 6700: 1.582960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6800: 1.607109 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6900: 1.584447 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 7000: 1.580931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "schuology michine a not nutura control for the lifictually amerimes and me gener\n",
      "wors he iingy wabe encources the curren world a sine second rycle tadks ruish cl\n",
      "hoots the latenchendr later taushite or discoultim for janing to be the expensis\n",
      "s and are world d paed i an externs the harms two zero six four there as inrive \n",
      "tilazh and economic than is severa zero three d one six two zero since sputeda o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    methodDict['Simple LSTM Model'] = float(np.exp(valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296874 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "p wou    lh ewk saqlh  nnocbezgeufeatv    smdl a  ow rnoae sszwraggjftrn  c wc v\n",
      "cn ere rqpzk p n e  uof yqeue  n saihbbdlaraeseop  kpmzcvipohtm r sbswloo ghhl s\n",
      "udtmxmpjuwjaskdsbfsngnq nsl  g euxyiyrnvi n  srhsuhopnauoagihle m la eknphrgciel\n",
      "eyoo hkqsatvbik lqozzdo rcbshueie smxvsp sb hy aidiu llh th uyfsxgpumsmtpw jhhmk\n",
      "ntnc hbmsjvatrb ynwuosp x t  o qsbtpahiy o ati mui  evzuipc aetdk mwh hl vq sopv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.589872 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.78\n",
      "Validation set perplexity: 10.83\n",
      "Average loss at step 200: 2.246951 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 300: 2.082324 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 400: 2.030088 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.975360 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.897103 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.867175 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.863884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.841428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1000: 1.842770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "================================================================================\n",
      "ist y frest courai f other coftus regiosaal vicanesh scon socdie s quiti ghree f\n",
      "verch placucath wide recond pie uphy s this transs been tablecks bw joougbing y \n",
      "k blows and namy as or are sueneby s deincired priband and and one two migght in\n",
      "vitial reblorbated recorder out in impanimattions abouk moted a cedenur sours to\n",
      "b brestally bask selvicily for poond iare its f of the a coond a stven partilati\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100: 1.797506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1200: 1.765582 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1300: 1.757559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.758010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1500: 1.745214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1600: 1.730237 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.713364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.687776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1900: 1.688347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2000: 1.674774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "k by to stucted mare ds singlo competymua and even term let and stial exheres th\n",
      "orient undezaly zero the mould playh feist to ceams their may a shissic socled c\n",
      "allo of bartions is in the blanked in diars or so heschant singer serok eight ze\n",
      "der expirentes and verse of oxserctlists sce thought of ds its posntess bembreon\n",
      "k yong of the firets sackia anj and zero zero fialloon c on call bit unfourled o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.681119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2200: 1.702158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.701632 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.681770 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2500: 1.687700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.669526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2700: 1.678986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2800: 1.680995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2900: 1.675998 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3000: 1.682774 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "cude of the barges pecirus three partions epsearri be dies exily ab a the be two\n",
      "eepan twansm to ted dana time tunter of the kejination cariesred comberized mean\n",
      "fruchused acvirsions reliue also aragesh krwon as slisheof presited from tounsie\n",
      "ines the news echosty hadrfew termann states air our the inferits two virring to\n",
      " of where the engmment daer the bact treats it ver s one nine six american s ill\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.651026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.639057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3300: 1.648275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3400: 1.632535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.672826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3600: 1.650878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3700: 1.651244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3800: 1.654861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3900: 1.651280 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4000: 1.637676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "mania sponies as catial some in measus the forded as formed gollicle a vear forc\n",
      "jenish and baser decains interfore firstery somiliag mace s a them and zero the \n",
      "zand forralthor the the lebsure mary with french ipsubclement auwhamal helled ma\n",
      "ass ack franta jock alset forcest and the foom usin fave ordereasy swiens at isd\n",
      "atching mather that eight a vare slats has as world because this size rnigies wr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4100: 1.616129 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.612645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4300: 1.618507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4400: 1.611520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4500: 1.643163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4600: 1.621728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4700: 1.625473 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.604032 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4900: 1.620800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.611558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "est of emendomic sponicist states in hegarly feating an bese marcal k now edraci\n",
      "lewina myered on monards writer s rebariesial for are windia peacing by was very\n",
      "or in the orse for the event which iilledosess the depialer the grou calbture th\n",
      "on its carmine laals armilly including pajeal was to bull mays plift excreisn vi\n",
      "nia cannaming traduarly a one nine five three vay one six seven three three sour\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.585671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5200: 1.591222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5300: 1.588091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5400: 1.585442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5500: 1.587999 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.558276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5700: 1.572038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5800: 1.597096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5900: 1.577908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.582272 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "an bines to more wnolling and higs places of basers at lear only roup lide or ca\n",
      "fa called sicial ii elizasorep peals area alway in takbises with vanues officerg\n",
      "ques x cantast performance gut have withyty machina increction model sometical j\n",
      "ph known purit uslable and of the conscienced aw show sams openmar not a lighted\n",
      "d euromy unred two majaly punarecrarmed ancenca exts that huscorty read deyerori\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6100: 1.570736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6200: 1.583985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6300: 1.581341 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6400: 1.572077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6500: 1.553186 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6600: 1.593529 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6700: 1.569679 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6800: 1.573894 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6900: 1.571555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 7000: 1.588109 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "poss two zero zentectume occines anown remormilly be an in the beneries larges i\n",
      "re the generallien at secrarie land are eight two zero six beginist ethnom not r\n",
      "a to abarrian from those other past british such as worll all the or been liated\n",
      "ver de runs was frank angle settletic rocage as the coogatives of jefus velemoni\n",
      "minance as since anology which as asparted they is parts a carder to canion inde\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  methodDict['Single Matrix'] = float(np.exp(valid_logprob / valid_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Validation set perplexity has increased sligthly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point a\n",
    "\n",
    "Let's first adapt the LSTM for a single character input with embeddings. The feed_dict is unchanged, the embeddings are looked up from the inputs. Note that the output is an array probability for the possible characters, not an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-02b24b6a6438>:58: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.317605 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.59\n",
      "================================================================================\n",
      "cofjce l tbunknircnrae yayttedfg w  kqyprsy iyi xusbf itt wr fowqe m vte tf se r\n",
      "ngrdlolna jcipu s ojoqfkpdafe suin eatyfuq  weoe skjjvrr czecug d i  zhe    zk j\n",
      " reewysn lp sux  rnd n wvansi ycye oncg la mol aiim d dvnszitnk nffbnns ifc kb  \n",
      "wsbbxjeekgdeonombue l tvwkkizo cstty hhgyogtymar ant wrnwst m lo jxidmybyjvk w h\n",
      "gn ignsljfaeenprjahuaazets  hrtho asinl    dvtcvcpx wx hesmaoba ljea lyaryttxtog\n",
      "================================================================================\n",
      "Validation set perplexity: 18.85\n",
      "Average loss at step 100: 2.296089 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.20\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 200: 2.015637 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 300: 1.917029 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 400: 1.859611 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 500: 1.879623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 600: 1.813929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 700: 1.798262 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 800: 1.789435 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 900: 1.787583 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.721233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "ly flip and then sequnce resunger lime with loge proce one five nine daked it er\n",
      "ference the leadials merty cleast a birmited drigue other a s roned one nine spe\n",
      "retion vilure of has y a knough resepended with nome enek secos an musce due pub\n",
      "bwoad ewege eight b se seperon deplainey were aurguazchia  mdbort from bi three \n",
      "quentional one nine six ters is or part inlicy for respublic cyments holligabe a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1100: 1.698281 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.736457 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1300: 1.710850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.690941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500: 1.685179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.684490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1700: 1.707099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1800: 1.673468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.681992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2000: 1.691990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "kingally cate born in securative bradian labballs equipesion of the fire procide\n",
      " tersive jowster edfatends list uses of koke in a one nine eight ourciginial uns\n",
      "ing growsacoson in beto chilliora the sefferst popelds whel liok aboverioa for p\n",
      "orkings bi book chogen collective of and they may godelowing fat do the raiks in\n",
      "zed has romesal came ballieves hausest she whites do see wall force rangernar us\n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.681086 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.653416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.664811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2400: 1.668906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2500: 1.694685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2600: 1.665967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2700: 1.684457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2800: 1.644662 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2900: 1.651927 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3000: 1.655160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "hers and rupsish accossof sidenc authornedijuction of three auting heal showerni\n",
      "x assing compack is aften ardicovan many high jichim oheigl solected nucleries e\n",
      "nese and pripa nuc sopectly unior outil where two wishique where out autit now w\n",
      "ing the purch forthming is worders d ther off june imporiat of lato refleptime f\n",
      "st outportimatelerss of the primt of dever insidlation outrusics used in one thr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100: 1.652536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3200: 1.653562 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3300: 1.634733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3400: 1.638298 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3500: 1.630190 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.632339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3700: 1.635397 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3800: 1.630257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3900: 1.625199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4000: 1.627196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "ly rile calengters in and inited kos ins such at of saird purposdw could was two\n",
      "videstative reversions of sever a offficiallitled as the scapain sunnisized to a\n",
      "tics in the firtwage such branives enthy tuded is respufcticular tobish number a\n",
      "ly with coutsiler valist former three one eight two the hight speacreted when wo\n",
      "man punse the deacres two two four peensers of remithine country one b that cost\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4100: 1.629011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4200: 1.621355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4300: 1.604634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4400: 1.628887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4500: 1.645103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.644993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4700: 1.608738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4800: 1.598490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.613780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5000: 1.635906 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "way jazin a basil sitiona was for the seve nine three yar a free beoing the orig\n",
      "ter zero sannted the tecoli impaom of on extender his be hed gnefitumes vasch th\n",
      "que in redlen workerff inicemus usos of submine it for achcent a m altaions the \n",
      "zard any ciller vertictions of gitare latle cimith choalia for called supportury\n",
      "x traditar to stristic elecordey varation set as interritions isbn in with hel s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.628988 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5200: 1.612348 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5300: 1.575391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5400: 1.572123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5500: 1.559336 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5600: 1.589005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5700: 1.545528 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5800: 1.552891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5900: 1.575658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.541451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "well trind luctuted it is a great one of the risobbler langly from distrogy the \n",
      "chry rock of even weis function attage anys canalsity at a why neanmet eacimiay \n",
      "que speciusate theory the baching not minivivizo conuited now hurch econes vote \n",
      "gent en years the cetic is ata gressept introduction mexiencal tasa many was unt\n",
      "ses so goants commationio poessions the one nine two six one zero eight three of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.558700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.578534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6300: 1.591924 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6400: 1.622442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6500: 1.611843 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6600: 1.581446 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.571100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6800: 1.554026 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6900: 1.548515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7000: 1.560059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "raters in contenk and the asz inforts as memers dall eight population rated stra\n",
      "uls shorting book the eight norgisy diental drevisting and success of anciunt of\n",
      "gent mebl base machint concerned in age eng of europeas flor or two zero zero ot\n",
      "rack two zero zero eight two one old locally levent ponious the probrabs pizen o\n",
      "ent stamberials and traonsport that eight nb slarile stationsled to mothish poti\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    methodDict['Embedding lookup'] = float(np.exp(valid_logprob / valid_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point b\n",
    "\n",
    "We can now use bigrams as inputs for the training. Here again, the feed_dict is unchanged, the bigram embeddings are looked up from the inputs. The output of the LSTM is still a probability array of the possible characters (not bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(train_labels, 0).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "       \n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298117 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "gplvvjuaeecuc e g rgg qrtlkpzgeqmekcuhiin kdmce   xqmgsctbe dsx e l anacdfcjpetir\n",
      "jivqtvolxe acleyiwyupxjfumnst iqa tsdttaedle o ewexhxqore tigstaxemjqrkueoqpkemfi\n",
      "v mem iw xeqdmdnmusynax t ybcjtxo ccofih hmtr s  zni yrqide veoqaoloirzlon  tu ep\n",
      "vuf cs iylinemhjcen t lrr brcpffsewsiss rfltmaewcmdhgsaeit jvpcij st x iosudb i  \n",
      "ygeemeeiodyumitbhce y hedvpap vp auh biobfy  ogqervlha yvqpmmbbxamor cis wgfv yqo\n",
      "================================================================================\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 100: 2.271931 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 200: 1.970014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 300: 1.888154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 400: 1.829205 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 500: 1.764832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 600: 1.762104 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 700: 1.744603 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 800: 1.727210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 900: 1.718061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1000: 1.693611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "eceives by hohould in one cents usand the by the mated mather one seven with the \n",
      "tpos who detice edition from only one zere nine two zero swi one nine from six th\n",
      "oxament of scienters the largance he pround is in eather presityer in that cellie\n",
      "trical decid by teleechcisaidly in the commons in the cland nat row univing enact\n",
      "fworking in speeks a nine a sus from mhat six one the may buylre crated the stoni\n",
      "================================================================================\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1100: 1.694583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1200: 1.691566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1300: 1.694874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 1400: 1.666125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1500: 1.652415 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1600: 1.648379 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1700: 1.649768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 1800: 1.672325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1900: 1.653977 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2000: 1.666126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "lhel three sing reconling chinasts as he of by rethely chick two europe seven to \n",
      "vv   eleight chartistrahes pose offerrend relation a leam set with ish may fid or\n",
      "equent caselfalenity attacces kin sident of the s aliteratives that the and the l\n",
      "nx selycks seven also has amenlate in shutter charty of the for         accul loc\n",
      "sza presist transted pouttink hydroths one four minethat and peder usen a nation \n",
      "================================================================================\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2100: 1.649128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 2200: 1.672871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2300: 1.649029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 2400: 1.647447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2500: 1.652664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2600: 1.646819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2700: 1.627350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2800: 1.628215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2900: 1.620447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3000: 1.641115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "kradificifes as links with to in arguates rogram century xml chichich song sia of\n",
      "xml milichii orderal laving touristic and theodosical governsm but inflegional se\n",
      "equenterizaar of imported one vile examility west for noquest with at s six form \n",
      "vv super can tradists ockery consistson of be and one nine eight five goal action\n",
      "nz or aberwebm wars of malise considering conserted and asial daw his are coechni\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 3100: 1.612361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3200: 1.625575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 3300: 1.623348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3400: 1.624534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 3500: 1.608920 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3600: 1.628261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3700: 1.602149 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3800: 1.598373 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3900: 1.590482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 4000: 1.609326 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "ojeycher valone internalte grammon as and which have a four to as an it sectory i\n",
      "xvatly rome doals ingermas and makeep and greshreec relabova variked to the losod\n",
      "most long them resourpole forms compameity s clos offeature of about for afntical\n",
      "hz hosf dalges wict is bear be algebraea of make the flowerbank bious floackeys b\n",
      "lreads adian in austrayestraeyty for yelon underned marchdmity unisscenses espane\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 4100: 1.619118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 4200: 1.603801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4300: 1.570154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4400: 1.596027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4500: 1.589853 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4600: 1.586339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4700: 1.598020 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 4800: 1.595132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 4900: 1.617378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5000: 1.626254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "jence carriety enterpan justpc helraily one languages and colvine and th centurie\n",
      " nine apwed to be up hendavaugaachundare islands caubut of contentated diaci main\n",
      "mbring of abasian species had chunrs     emxicilitepopersion isence are mre gener\n",
      "yx walking majories into the united mainte its it state built adal social scribec\n",
      "wfun my unlivy effrole agrief some d resultude it of comparol machinenter and a s\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 5100: 1.589375 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5200: 1.592903 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 5300: 1.567772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 5400: 1.558748 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5500: 1.557292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5600: 1.543011 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5700: 1.577044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5800: 1.566937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 5900: 1.570302 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6000: 1.532167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "by the oper seven zero are descrein browns studious estime imseal spachiness pred\n",
      "oppo the did hange for dations of this augutei rast swinst pate yard cufficial di\n",
      "btune concerniatius of e the rairconver one nine six faskasher unths at further e\n",
      "yhis hores the elic defenses yoursally canado engineer to delles sultistramiaa un\n",
      "swested bean one six nine two zero zero zero zero zerom nine zero three one nine \n",
      "================================================================================\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6100: 1.583210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6200: 1.583215 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 6300: 1.566656 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6400: 1.582257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6500: 1.579722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 6600: 1.568582 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6700: 1.559420 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6800: 1.571815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6900: 1.605795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 7000: 1.584769 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "xuissis a matholic with a several until runn made one nine three and helcyfer cor\n",
      "ijan every were principalays for the first viologuyles if first stally envisions \n",
      "mentia carte muments prepience guills through forcot a area southern to demost s \n",
      "south law is can publishsly could it six six poside woder power two two flose pro\n",
      "bjectary enputed grounded make contan the location press the claruline title to h\n",
      "================================================================================\n",
      "Validation set perplexity: 6.63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    methodDict['Bigrams'] = float(np.exp(valid_logprob / valid_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It works, but the validation perplexity is a bit worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point c (Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "      \n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304331 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.23\n",
      "================================================================================\n",
      "ldeeedliy hztmr rsepety v kdn e grvv a qgvb eq  kmrbcree yxb gmwactdcdie bez agxc\n",
      "z reeeici ma ysbdbof zoem lpsviiesqx dejfhe eiz x eajed pjes  wh ab  bj w d efcow\n",
      "zy r ejwtnwnie iznzlivrtfe mbdihof pbsowcv tgn nio qzobvki evd evda pe iy es lea \n",
      "lpcafbp dkkhodiglahigl  dzqbcaaats s jfwpye nppbodysepkyg behqqshffsm w arosd  we\n",
      "dqkrossakf  btci mtrweanmzeeeqsoeuirtjsmgs veju t  grrnz zsyaqknegchfpv  zdse cef\n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.297724 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 200: 1.969694 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 300: 1.875615 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 400: 1.823458 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 500: 1.801889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 600: 1.756067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 700: 1.752208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 800: 1.712383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 900: 1.707337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1000: 1.698947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "qxery as recorder than temperiogse hators of und less in chand hago be which writ\n",
      "yy reasonary in the turicys for  decontings contons of kinglost outh one nine zer\n",
      "med forms readausity biate in recention a perhlatic seoped ally of died heall nix\n",
      "city was actronce of cyrns and sellience in melorge the fiely cyms is acteride st\n",
      "sqrmethas engoned on whike one two five six four losock staid clories g arbirt ho\n",
      "================================================================================\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1100: 1.693210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 1200: 1.684236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1300: 1.668114 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1400: 1.671026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 1500: 1.693261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 1600: 1.680445 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 1700: 1.658528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1800: 1.687656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 1900: 1.686179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2000: 1.653313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "================================================================================\n",
      "fc conference paprist of agents access advue crearius rations that astelengin the\n",
      "tion and in one earn iii in one eighlone instatical the less facts rappes s like \n",
      "year lved noting irrial was dictoria needraw accvdical as assumpating canimseling\n",
      "tformer also igbs which also arbision resultor a can caspencircleigloch at event \n",
      "ljoink moreen loess august destilii concenture protmethis fsz hundign grawe one n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2100: 1.642415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2200: 1.629014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 2300: 1.671145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2400: 1.657146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2500: 1.633926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2600: 1.621404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2700: 1.620091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2800: 1.633500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 2900: 1.606873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 3000: 1.606305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "xlow purdind to is posit by englanima can occult was gree other few colong the bu\n",
      "ii used involved formish engpyth citity when are are empire political defet spect\n",
      "where of seen reproducti impare proted parts or using ached are of general presou\n",
      "bgrade things elect idead cerail kan or lence of famizer cult and solko black ban\n",
      "jrdeletters this limitales expericat which eight five zero zone firman asfours he\n",
      "================================================================================\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 3100: 1.631011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 3200: 1.627434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3300: 1.618453 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3400: 1.618639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 3500: 1.603014 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 3600: 1.580937 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 3700: 1.595077 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3800: 1.610486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 3900: 1.618066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 4000: 1.602510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "wponent of one seven paracters his one two  ens in the music port toring not to d\n",
      "xperial s cowerversion computerioses aracture was mos is finalendence stact of an\n",
      "jderie systen wibatar uniew els from pointilure one nine five five revolved by th\n",
      "opfe and broheater tandrus carellians of and his synhat long wai blacks was still\n",
      "tpu to is national south with one nine three six five subsants sta two ustlemes a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4100: 1.613149 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4200: 1.590458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 4300: 1.594809 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4400: 1.605403 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4500: 1.606182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4600: 1.596770 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 4700: 1.593089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4800: 1.610607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 4900: 1.598485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5000: 1.612865 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "mcq of the dalbum and high emperation that the eds way have a glew comethree two \n",
      "izi with bel empire and the partiell of the tocates the hoters hating k dimend wi\n",
      "bc astra shotgfames majaone eeps sepfected want debtfet cese of a ve them make wh\n",
      "duffbion majorical ment work is a state any furn years responsible of might not p\n",
      "yf is the plant and one pe sames stylot relation two zero zero as that i harvaria\n",
      "================================================================================\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 5100: 1.601968 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 5200: 1.598673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 5300: 1.594431 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 5400: 1.579794 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5500: 1.586377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5600: 1.607370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 5700: 1.586066 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5800: 1.577933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 5900: 1.586531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 6000: 1.595809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "yffs worked this final musical seeme that amegres downlation fieve undernal relat\n",
      "qd arabo the first after s cup one seven and k one six five four five eight obsai\n",
      "qyoung seven able he made woot of advential neater sized a firformation a states \n",
      "hvresiw postectabarder when they and firsts wellion eight named a milight conflin\n",
      "vq inf ouldenting and as home blavorador in langol state relet imulptive on in th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 6100: 1.615862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 6200: 1.594097 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 6300: 1.604457 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 6400: 1.624860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6500: 1.639343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 6600: 1.609749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 6700: 1.606172 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 6800: 1.595832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 6900: 1.560953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 7000: 1.603087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "kbig a late has four nine four dish woundasions coast its of al and in the meving\n",
      "ared og a nology marties the way loure profield no the grainal tell to the pointa\n",
      "epaster speakumbering nund in by the city duline pant itsilurago of films to allo\n",
      "ybory popons cropyribility flam on memor canaside was a celomenty to bioice vaker\n",
      "base coills isfilling traditions to d philosonial strasius not s and runagents of\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7100: 1.596349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 7200: 1.592929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 7300: 1.602761 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 7400: 1.589952 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 7500: 1.592252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 7600: 1.576975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7700: 1.591154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 7800: 1.604019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 7900: 1.609720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8000: 1.607029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ly is one and team bigrapace actors has resident of the from shown to hijacks his\n",
      "lxist paular statastrol and such aspects court is inforrosed and wouls were and o\n",
      "pfourf decemd has the and rom engressoms cdly wir de present the kurn baseurnia i\n",
      "uffered to totables ejg imaged a abduct engineres and homodertainly to new events\n",
      "pn were bandard or a refuse can its conspirically count also was american ry lare\n",
      "================================================================================\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 8100: 1.580589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 8200: 1.588739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 8300: 1.598074 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8400: 1.596232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 8500: 1.608919 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 8600: 1.610350 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 8700: 1.596576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 8800: 1.609729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 8900: 1.592233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 9000: 1.600645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "yfg eving for its lattion to employmobads the concembotes standu amaughterbans co\n",
      "yas of denwitcher esce and most the bevelos indiacy five fertine joint to northys\n",
      "mvitan currene for pressive of the plant of who six dialecism to currence and an \n",
      "lzs han pecting booka africare zerolly motter system mumps by four when languh po\n",
      "zq as been age in shopport the couple manywas espontaken three true mal to to tho\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 9100: 1.599741 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 9200: 1.625412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 9300: 1.614710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 9400: 1.600401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 9500: 1.608424 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 9600: 1.605239 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 9700: 1.608646 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 9800: 1.608928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 9900: 1.571253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 10000: 1.588417 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "ob in one one nine one seven four zero zero him b most zenine nine four sm bettin\n",
      "wmands can nsless weensm of shed southeat whole plandus com indiam erge as pronum\n",
      "jbi on his plute which a required the exchers an one eight zero five can that fis\n",
      "ugh large some dayths links determalliable wallies barria only hubbard protondogn\n",
      "vq an its to known desies than obal recouncil cobnt and ropors countial late a th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 10100: 1.607690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 10200: 1.601681 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 10300: 1.597714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 10400: 1.602834 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 10500: 1.620632 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 10600: 1.563333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 10700: 1.570144 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 10800: 1.594203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 10900: 1.604478 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 11000: 1.575137 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "jforces brounkwans with he campcislimined j isborg are of hillional timents of tw\n",
      "there chember but koneach is one eight nine one jifters one two is elesse keyudig\n",
      "gs than creatruced tolocking iliam the two during disconcrea rated lywood dists s\n",
      "lned on akin the ocnals sa  accordel expendiand a s fichysical largeduces of cacc\n",
      "fe capturnales yorilebraid the onemdread central algets gegion acadom by it with \n",
      "================================================================================\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 11100: 1.560998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 11200: 1.570336 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 11300: 1.554538 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 11400: 1.564335 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 11500: 1.568469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 11600: 1.536899 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 11700: 1.539641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 11800: 1.567788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 11900: 1.555497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 12000: 1.539739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "xjne sens a flemgs heater the ethine are the la neeces thump descully a new inter\n",
      "rns for helkisions of thomap zeror formed take incleved usual trowan their trines\n",
      "pvde one nine six within the his the ethin african pludge or salation sateen also\n",
      "c ar austrine seven three st incales union shed other doiltc example the caucauce\n",
      "obutury taihinne off evers scienthelbal civy in mixe of prize a marced s the form\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 12100: 1.542723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 12200: 1.566754 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 12300: 1.555376 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 12400: 1.590891 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 12500: 1.566274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 12600: 1.553178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 12700: 1.553008 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 12800: 1.567346 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 12900: 1.595167 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 13000: 1.564454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "pq the more judde holder people this berfuldbeign the rend proka adepting the lan\n",
      "mnet that years fy many at one nine zero zero three renternant which a the almosi\n",
      "qyalas three zero s rah s strator real decific times had owelrans at islame clev \n",
      "vs of addhusearch forea of between and constitute of repuming nexpof and sales an\n",
      "eight s reflings electrong of the roman and rder descrist ancient marriods inves \n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 13100: 1.560244 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 13200: 1.606956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 13300: 1.584300 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 13400: 1.590274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 13500: 1.606187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 13600: 1.587575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 13700: 1.560586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 13800: 1.543467 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 13900: 1.567668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 14000: 1.561771 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "================================================================================\n",
      "mongriengigh alphabet fant belooses us bob jekdozardder journal contriyal canon f\n",
      "equentiges by kai recognies elemens valocion incrediudger on nometronment legal h\n",
      "eould a receificted would ladultially open to moident leadersony birved electroli\n",
      "pical defying the american communiscreateged miss of one nine nine varianne pary \n",
      "jr of vania understate family from the are eight one eight seven set to it limita\n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 14100: 1.580714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 14200: 1.585090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 14300: 1.574631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 14400: 1.584736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 14500: 1.610380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 14600: 1.590193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 14700: 1.602343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 14800: 1.592653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 14900: 1.583544 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 15000: 1.583983 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "qe wheassaking acynal jund of these to a torders insies accept in a s when qir co\n",
      "furthin kha as with a presencektd by is beginning a mouth rolebra storbia century\n",
      "sions desin of aceuphonst solutions of conscript on confrom the preview one nine \n",
      "jc not revious medied which holading mie drivatics of characuly azyuka volitation\n",
      "jbloluse by confet body great famou security in the ssource but fer two ro howevh\n",
      "================================================================================\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 15100: 1.545405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 15200: 1.564308 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 15300: 1.540308 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 15400: 1.547462 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 15500: 1.511778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 15600: 1.524689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 15700: 1.511642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 15800: 1.503367 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 15900: 1.517865 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 16000: 1.529465 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "tudieldes reduction prinaly palan with your settlat two zero one nine one eight s\n",
      "cred original extensions of geography sards so km the coloral date that him flori\n",
      "iyears or as entries or mathemated in a form rayday that violence smitted econob \n",
      "tmetic to cardlateration first authority and has begule such as drication the ran\n",
      "xy beings for storants protestly ownee one nine mage us says were news of the pas\n",
      "================================================================================\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 16100: 1.524129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 16200: 1.491495 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 16300: 1.474549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 16400: 1.514772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 16500: 1.526591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 16600: 1.524268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 16700: 1.556185 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 16800: 1.504358 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 16900: 1.523456 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 17000: 1.531571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "or one nine eight eight fiveum in oction of white an in different layed most dode\n",
      "dp there which region to assoversy ambox style lumical are devices the is one act\n",
      "human inities majorine example  analies the being spcky of the called as of meish\n",
      "yy good were designuxich it iii are in bel self the phs one firseven two eight an\n",
      "hwar its system one in eufficial politicians in an economical malan step into sla\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 17100: 1.517577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 17200: 1.542892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 17300: 1.550136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 17400: 1.592761 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 17500: 1.566120 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 17600: 1.587908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 17700: 1.581760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 17800: 1.561800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 17900: 1.556717 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 18000: 1.528264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "================================================================================\n",
      "vuld by the part the new scentually cup for the malants backe themselved translat\n",
      "during foculzan rusus as the son nover economic two three five ths nove that some\n",
      "xznan in the idea style if cure their musical worker simplays and proceinfortuad \n",
      "vww is performed them nine nine eight ince and marriazney of metite left laken b \n",
      "equality service tostrove productions one nine nine one six then congreposed they\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 18100: 1.515027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 18200: 1.540071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 18300: 1.543438 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 18400: 1.574948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 18500: 1.566272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 18600: 1.573182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 18700: 1.565687 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 18800: 1.567155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 18900: 1.551377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 19000: 1.598138 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "aposite the music simpled of the waypstity the battils of of vieqldon from thirk \n",
      "ecentus jegions and and all allowed known de intent and two much reben accept fic\n",
      "jb three mimited equisterative hokuid two zero six zero two soical as internates \n",
      "mg one compositional nine she anakist head his ensions only his reference the rev\n",
      "sz commitation stpty and imposed as bed at the may and macks station producyi as \n",
      "================================================================================\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 19100: 1.578514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 19200: 1.560891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 19300: 1.561062 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 19400: 1.538258 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 19500: 1.539773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 19600: 1.548005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 19700: 1.559604 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 19800: 1.539201 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 19900: 1.548146 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 20000: 1.519825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "bz boa as written capctices contain argends its you morp stripe and the trau like\n",
      "qh antirabies the in for mlit his at the united in on the manufark as and charant\n",
      "as the sextopbatabics a modern cause west than behetienlam sier diviside oric asp\n",
      "gks parts speight such amers vagica specified special rows brizary na thousocial \n",
      "zsmists brahirptoo some persons beure owneral arty ariste and developed to have b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 20100: 1.525319 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 20200: 1.529744 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 20300: 1.552675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 20400: 1.548309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 20500: 1.545498 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 20600: 1.516832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 20700: 1.508099 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 20800: 1.524753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 20900: 1.519209 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 21000: 1.522531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "ful were the symbol popular homerage additorics flow of the usefore broave inner \n",
      "hat believer to devised llof hopd hents of greek articulartronied for came poland\n",
      "q k irani ways ungus of the forum a heter the mounts is ray and even agences toky\n",
      "zygowlatery direy with after that the north that leading edwarnist active the chu\n",
      "us g to wists was hermatifitus one in has language for not publied practions here\n",
      "================================================================================\n",
      "Validation set perplexity: 6.67\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    methodDict['Dropout'] = float(np.exp(valid_logprob / valid_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting method success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAHaCAYAAADG5hVZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu4XFV9//H3h5sgEUFuCuEuAuEWIAgqPwxSFCxVS1FB\nKWIFxFtFLLbVCoi2aqVWvBXRqlgtqVJtERBBIQhWQQIpNwUqIiAqogYICCTh+/tjJnAScjk7OTP7\nzJz363nOk9lrz8z+ztk5yeesvfZaqSokSZIkjd4qbRcgSZIkDRpDtCRJktSQIVqSJElqyBAtSZIk\nNWSIliRJkhoyREuSJEkNGaIlaQil4wtJfp/kqhaOf0qSL4/Rex2V5IqxeC9JGiuGaEkTRpJ9kvxP\nkvuS/C7J95Ps2XZdPbIPcAAwuaqe28sDJZme5K5eHkOSxpvV2i5AkvohyTrAecCbgK8CawD/D3ik\nzbp6aAvg9qp6sO1CJGkY2RMtaaJ4DkBVnV1VC6rqD1V1UVVdB08efpBkyySVZLXu9jO6wyPu7g6R\n+K8Rz315ktlJ7k/y0yQHdtufnuRfk/wyyS+SfCDJqt19z05yWbdX/N4k/9FtT5J/TnJP9/2uT7LT\nkj5Qkk2SnNvtVf+/JMd0298AfA54XpK5Sd63hNce1e2J/+ckc5LcluT53fY7u8d/3YjnPyXJaUnu\nSPLrJGckWSvJ2sC3gE26x5qbZJPuy9ZI8qUkDyS5Mcm0Ee+3Q5KZ3WPfmORlI/at3/1c93eHomwz\nYt+ovz+S1EuGaEkTxS3AgiRnJTkoyXoNX/9vwFOBHYGNgH8GSPJc4EvAicC6wL7A7d3XfBGYDzwb\n2A14MXB0d9/7gYuA9YDJwCe67S/uvsdzgKcDrwJ+u5SaZgB3AZsAhwL/kORFVfWvwHHAD6pqUlWd\nvJTX7wVcB6wP/Hv3/fbs1nsE8Mkkk7rP/VC3pqnd/ZsCJ3V7ug8C7u4ea1JV3d19zcu677kucC7w\nye73bHXgm93PvxHwNuArSbbrvu5TwMPAs4C/6H4t1OT7I0k9Y4iWNCFU1f10xgkX8FngN93ezo2X\n99okz6ITFI+rqt9X1byquqy7+w3A56vq4qp6rKp+UVU/6b7vS4Hjq+rBqrqHTvA+rPu6eXSGXGxS\nVQ9X1RUj2p8GbA+kqn5cVb9cQk2bAS8A/rr7+tl0ep+PbPBt+VlVfaGqFgD/AWwGnFpVj1TVRcCj\nwLOTBDgWeEdV/a6qHgD+YcRnWZorquqC7vv/G7Brt31vYBLwoap6tKouoTPU5vBuT/2f0Q3oVXUD\ncNaI9xzV90eSes0QLWnC6Aauo6pqMrATnR7cj43ipZsBv6uq3y9l30+X0L4FsDrwy+6QhTnAZ+j0\nvAK8CwhwVXc4w190a7yETo/tp4B7kpzZHc+9uE26NT0wou3ndHqIR+vXIx7/oXv8xdsmARvS6YWf\nNeKzXNhtX5ZfjXj8ELBmd3jMJsCdVfXYEmrfkM79Oncuto9ufaP9/khSTxmiJU1IVfUTOsMtFo6n\nfZBOUFzomSMe3wk8I8m6S3irOxkxZnex9keADapq3e7XOlW1Y/f4v6qqY6pqE+CNwKeTPLu77+NV\ntQcwhc6whROX8P53d2t62oi2zYFfLOtzr6B76QTqHUd8lqdX1cKhHtXw/e4GNksy8v+ghbX/hs4Q\nmM0W2/e4UX5/JKmnDNGSJoQk2yd5Z5LJ3e3NgMOBH3afMhvYN8nmSZ4O/O3C13aHC3yLTtBdL8nq\nSfbt7v5X4PVJ9k+ySpJNk2zffc1FwD8lWae7b5skL+we/5ULawF+TyeIPpZkzyR7dccNP0hnbPDI\nHtuFNd0J/A/wwSRrJtmFztCSMZmbebFjPUZnCMw/J9moW/+mSV7SfcqvgfW737fRuJJOz/S7ut/L\n6cCfADO6Qz++DpyS5KlJpgAjb3Ac1fdHknrNEC1poniAzo10VyZ5kE54vgF4J0BVXUxnXPB1wCw6\nY3RH+nM643F/AtwDHN993VXA6+mMd74PuIzOUA7ojE9eA7iJTlA+h87NctC5ge/KJHPp3HT39qq6\nDViHTmD9PZ1hDL8FPrKUz3Q4sCWdnt1vACdX1XcafVdG76+B/wN+mOR+4DvAdvB4r/7ZwG3d4R6b\nLP1toKoepROaD6LTy/1p4Mju+wC8lc4wkl/RuVrwhREvb/L9kaSeSVXTq3CSJEnSxGZPtCRJktRQ\nz0J0ks93J8O/YUTbM5JcnOTW7p9N52mVJEmSWtfLnugvAgcu1vY3wHeralvgu91tSZIkaaD0dEx0\nki2B86pqp+72zcD0qvpld/GCmVW13TLeQpIkSRp3+j0meuMRK0v9CljuSmGSJEnSeLNaWweuqkqy\n1G7wJMfSWWaWtdZaa4/NNttsaU+VeOyxx1hlFe+THXae5+HnOZ4YPM8TwyCe51tuueXeqlreaqxA\n/0P0r5M8a8RwjnuW9sSqOhM4E2DatGl19dVX96tGDaCZM2cyffr0tstQj3meh5/neGLwPE8Mg3ie\nk/x8tM/t968H5/LEylOvA/67z8eXJEmSVlovp7g7G/gBsF2Su5K8AfgQcECSW4E/6m5LkiRJA6Vn\nwzmq6vCl7Nq/V8eUJEmS+mGwRntLkiRJ44AhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJ\nkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAtSZIkNWSIliRJkhoyREuSJEkNGaIlSZKk\nhgzRkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYkSZIaMkRLkiRJDRmiJUmSpIYM\n0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGS\nJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAtSZIkNWSIliRJkhoyREuSJEkNGaIlSZKkhgzRkiRJ\nUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYkSZIaMkRLkiRJDRmiJUmSpIYM0ZIkSVJD\nhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGSJElSQ4Zo\nSZIkqaFWQnSSdyS5MckNSc5OsmYbdUiSJEkrou8hOsmmwF8C06pqJ2BV4LB+1yFJkiStqLaGc6wG\nrJVkNeCpwN0t1SFJkiQ11vcQXVW/AE4D7gB+CdxXVRf1uw5JkiRpRaWq+nvAZD3gP4FXA3OArwHn\nVNWXF3vescCxABtvvPEeM2bM6GudGixz585l0qRJbZehHvM8Dz/P8cTgeZ4YBvE877fffrOqatpo\nnrtar4tZgj8CflZVvwFI8nXg+cAiIbqqzgTOBJg2bVpNnz69z2VqkMycORP/jgw/z/Pw8xxPDJ7n\niWHYz3MbY6LvAPZO8tQkAfYHftxCHZIkSdIKaWNM9JXAOcA1wPXdGs7sdx2SJEnSimpjOAdVdTJw\nchvHliRJklaWKxZKkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC1JkiQ1\nZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAtSZIkNWSI\nliRJkhoyREuSJEkNGaIlSZKkhgzRkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYk\nSZIaMkRLkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC2NkTlz5nDooYey\n/fbbs8MOO/CDH/zgSc+ZOXMmU6dOZccdd+SFL3whADfffDNTp059/GudddbhYx/7WL/LlyRJDazW\ndgHDbs6cORx99NHccMMNJOHzn/88z3ve8xZ5zsyZMzn++OOZN28eG2ywAZdddhkPP/ww++67L488\n8gjz58/n0EMP5X3ve19Ln0Kj8fa3v50DDzyQc845h0cffZSHHnpokf1z5szhzW9+MxdeeCGbb745\n99xzDwDbbbcds2fPBmDBggVsuumm/Omf/mnf65ckSaNniO6xFQ1WT3nKU7jkkkuYNGkS8+bNY599\n9uGggw5i7733buNjaDnuu+8+vve97/HFL34RgDXWWIM11lhjkef8+7//O4cccgibb745ABtttNGT\n3ue73/0u22yzDVtssUXPa5YkSSvO4Rw9tDBYveENbwA6wWrddddd5DlLC1ZJmDRpEgDz5s1j3rx5\nJOlj9WriZz/7GRtuuCGvf/3r2W233Tj66KN58MEHF3nOLbfcwu9//3umT5/OHnvswZe+9KUnvc+M\nGTM4/PDD+1W2JElaQYboHlrZYLVgwQKmTp3KRhttxAEHHMBee+3V74+gUZo/fz7XXHMNb3rTm7j2\n2mtZe+21+dCHPvSk58yaNYvzzz+fb3/727z//e/nlltueXz/o48+yrnnnssrX/nKfpcvSZIaMkT3\n0MoGq1VXXZXZs2dz1113cdVVV3HDDTe08TE0CpMnT2by5MmP/6Jz6KGHcs011zzpOS95yUtYe+21\n2WCDDdh333353//938f3f+tb32L33Xdn44037mvtkiSpOUN0D41FsAJYd9112W+//bjwwgv7Vrua\neeYzn8lmm23GzTffDHTGNk+ZMmWR57z85S/niiuuYP78+Tz00ENceeWV7LDDDo/vP/vssx3KIUnS\ngDBE99DKBKvf/OY3zJkzB4A//OEPXHzxxWy//fZ9/wwavU984hO89rWvZZdddmH27Nm8+93v5owz\nzuCMM84AYIcdduDAAw9kl1124bnPfS5HH300O+20EwAPPvggF198MYccckibH0GSJI2Ss3P02MJg\n9eijj7L11lvzhS984fFQddxxxy0SrFZZZZXHg9V1113H6173OhYsWMBjjz3Gq171Kg4++OCWP42W\nZerUqVx99dWLtB133HGLbJ944omceOKJT3rt2muvzW9/+9ue1idJksaOIbrHVjRY7bLLLlx77bU9\nr08rxwlT+uvSS9uuQJKkDkN0jxiu+stwJUmS+skx0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrI\nEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAt\nSZIkNWSIliRJkhoyREuSJEkNGaIlSZKkhloJ0UnWTXJOkp8k+XGS57VRhyRJkrQiVmvpuKcDF1bV\noUnWAJ7aUh2SJElSY30P0UmeDuwLHAVQVY8Cj/a7DkmSJGlFpar6e8BkKnAmcBOwKzALeHtVPbjY\n844FjgXYeOON95gxY0Zf61xZs2a1XcHEst12c5k0aVLfj+t57q+2zrP6Z+5cz/FE4HmeGAbxPO+3\n336zqmraaJ7bRoieBvwQeEFVXZnkdOD+qnrv0l4zbdq0uvrqq/tW41hI2q5gYrn00plMnz6978f1\nPPdXW+dZ/TNzpud4IvA8TwyDeJ6TjDpEt3Fj4V3AXVV1ZXf7HGD3FuqQJEmSVkjfQ3RV/Qq4M8l2\n3ab96QztkCRJkgZCW7NzvA34SndmjtuA17dUhyRJktRYKyG6qmYDoxpvIkmSJI03rlgoSZIkNTTq\nEJ1krRHjmCVJkqQJa1QhOsmfALOBC7vbU5Oc28vCJEmSpPFqtD3RpwDPBebA42Oat+pRTZIkSdK4\nNtoQPa+q7lusrb+rtEiSJEnjxGhn57gxyWuAVZNsC/wl8D+9K0uSJEkav0bbE/02YEfgEeBs4H7g\n+F4VJUmSJI1no+qJrqqHgPd0vyRJkqQJbVQhOsk3efIY6PuAq4HPVNXDY12YJEmSNF6NdjjHbcBc\n4LPdr/uBB4DndLclSZKkCWO0NxY+v6r2HLH9zSQ/qqo9k9zYi8IkSZKk8Wq0PdGTkmy+cKP7eFJ3\n89Exr0qSJEkax0bbE/1O4IokPwVCZ6GVNydZGzirV8VJkiRJ49FoZ+e4oDs/9PbdpptH3Ez4sZ5U\nJkmSJI1To+2JBtgW2A5YE9g1CVX1pd6UJUmSJI1fo53i7mRgOjAFuAA4CLgCMERLkiRpwhntjYWH\nAvsDv6qq1wO7Ak/vWVWSJEnSODbaEP2HqnoMmJ9kHeAeYLPelSVJkiSNX6MdE311knXpLKwyi87C\nKz/oWVWSJEnSODba2Tne3H14RpILgXWq6rrelSVJkiSNX6MazpHkuwsfV9XtVXXdyDZJkiRpIllm\nT3SSNYGnAhskWY/OQisA6wCb9rg2SZIkaVxa3nCONwLHA5vQGQu9METfD3yyh3VJkiRJ49YyQ3RV\nnQ6cnuRtVfWJPtUkSZIkjWujvbHwE0meD2w58jWuWChJkqSJaLQrFv4bsA0wG1jQbS5csVCSJEkT\n0GjniZ4GTKmq6mUxkiRJ0iAY7YqFNwDP7GUhkiRJ0qAYbU/0BsBNSa4CHlnYWFUv60lVkiRJ0jg2\n2hB9Si+LkCRJkgbJqIZzVNVlwO3A6t3HPwKu6WFdkiRJPbdgwQJ22203Dj744CXu/+pXv8qUKVPY\ncccdec1rXrPIvvvvv5/Jkyfz1re+tR+lapwZ7ewcxwDHAs+gM0vHpsAZwP69K02SJKm3Tj/9dHbY\nYQfuv//+J+279dZb+eAHP8j3v/991ltvPe65555F9r/3ve9l33337VepGmdGe2PhW4AX0FmpkKq6\nFdioV0VJkiT12l133cX555/P0UcfvcT9n/3sZ3nLW97CeuutB8BGGz0RfWbNmsWvf/1rXvziF/el\nVo0/ow3Rj1TVows3kqxGZ55oSZKkgXT88cfzj//4j6yyypLj0C233MItt9zCC17wAvbee28uvPBC\nAB577DHe+c53ctppp/WzXI0zo72x8LIk7wbWSnIA8Gbgm70rS5IkqXfOO+88NtpoI/bYYw9mzpy5\nxOfMnz+fW2+9lZkzZ3LXXXex7777cv311/PlL3+Zl770pUyePLm/RWtcGW2I/hvgDcD1wBuBC4DP\n9aooSZKkXvr+97/PueeeywUXXMDDDz/M/fffzxFHHMGXv/zlx58zefJk9tprL1ZffXW22mornvOc\n53Drrbfygx/8gMsvv5xPf/rTzJ07l0cffZRJkybxoQ99qMVPpH4b7XCOtYDPV9Urq+pQ4PPdNkmS\npIHzwQ9+kLvuuovbb7+dGTNm8KIXvWiRAA3wile84vFe6nvvvZdbbrmFrbfemq985Svccccd3H77\n7Zx22mkceeSRBugJaLQh+rssGprXAr4z9uVIkiS156STTuLcc88F4CUveQnrr78+U6ZMYb/99uMj\nH/kI66+/fssVarwY7XCONatq7sKNqpqb5Kk9qkmSJKlvpk+fzvTp0wE49dRTH29Pwkc/+lE++tGP\nLvW1Rx11FEcddVSPK9R4NNqe6AeT7L5wI8kewB96U5IkSdLYS/zq59ewG22IfjvwtSSXJ7kC+A/A\n5XkkTUjLWuHsjDPOYOedd2bq1Knss88+3HTTTYvsv+OOO5g0aZJTY0nSgFvucI4kqwBrANsD23Wb\nb66qeb0sTJLGq2WtcPaa17yG4447DoBzzz2XE0444fG5ZQFOOOEEDjrooL7VKknqjeX2RFfVY8Cn\nqmpeVd3Q/TJAS5qQlrfC2TrrrPP44wcffJCMuKb5X//1X2y11VbsuOOOPa9TktRbo56dI8mfJRNh\nhIskLd3yVjgD+NSnPsU222zDu971Lj7+8Y8DMHfuXD784Q9z8skn96tUSVIPjTZEvxH4GvBokvuT\nPJDkydcxJWmIjVzhbFne8pa38NOf/pQPf/jDfOADHwDglFNO4R3veAeTJk3qR6mSpB4b1RR3VfW0\nXhciSePdaFY4G+mwww7jTW96EwBXXnkl55xzDu9617uYM2cOq6yyCmuuuSZvfav3aEvSIBpVT3Q6\njkjy3u72Zkme29vSJGl8Gc0KZ7feeuvjj88//3y23XZbAC6//HJuv/12br/9do4//nje/e53G6DH\nuWXNwvK9732P3XffndVWW41zzjlnkX133HEHL37xi9lhhx2YMmUKt99+e58qltRPo11s5dPAY8CL\ngPcDc4FPAXv2qC5JGhgnnXQS06ZN42Uvexmf/OQn+c53vsPqq6/Oeuutx1lnndV2eVpBy5qFZfPN\nN+eLX/ziEqcqPPLII3nPe97DAQccwNy5c5c5fl7S4BptiN6rqnZPci1AVf0+yRo9rEuSxrWlrXB2\n+umnL/e1p5xySo+q0lhZOAvLe97zniWuVrflllsCPCkg33TTTcyfP58DDjgAwDHw0hAb7a/H85Ks\nChRAkg3p9ExL0tBre9WvifQ1XoxmFpYlueWWW1h33XU55JBD2G233TjxxBNZsGBBj6qU1KbR/uvw\nceAbwEZJ/h64AviHnlUlSVJLRjsLy5LMnz+fyy+/nNNOO40f/ehH3HbbbXzxi18c+yIltW5UIbqq\nvgK8C/gg8EvgFVX1tV4WJklSGxbOwrLlllty2GGHcckll3DEEUeM6rWTJ09m6tSpbL311qy22mq8\n4hWv4JprrulxxZLasMwQnWTNJMcn+STwQuAzVfXJqvpxf8qTJKm/RjMLy9LsueeezJkzh9/85jcA\nXHLJJUyZMqWX5UpqyfJ6os8CpgHXAwcBT74NWZKkCeCkk07i3HPPBeBHP/oRkydP5mtf+xpvfOMb\nH1/KfdVVV+W0005j//33Z+edd6aqOOaYY9osW1KPLG92jilVtTNAkn8Frup9SZIk9d+Sb2ycDkzv\n7jt1RPuewF0APPQQ/O53I19/AHAdADfcAM5y+GSXXtp2BdLKW15P9LyFD6pqfo9rkSRJkgbC8nqi\nd02ycJb5AGt1twNUVa2zogfuTpl3NfCLqnryclCSJEnSOLXMEF1Vq/bw2G8HfgyscBCXJEmS2tDK\nWqRJJgN/DHyujeNLkiRJK6OVEA18jM680656KEmSpIGTqurvAZODgZdW1ZuTTAf+akljopMcCxwL\nsPHGG+8xY8aMvta5smbNaruCiWW77eYyadKkvh/X89xfnufh19Y5Bs9zP/mzPDG0+fO8ovbbb79Z\nVTVtNM9tI0R/EPhzYD6wJp0x0V+vqqUuBzVt2rS6+uqr+1Th2FjyVEnqlUsvncn06dP7flzPc395\nnodfW+cYPM/95M/yxNDmz/OKSjLqEN334RxV9bdVNbmqtgQOAy5ZVoCWJEmSxpu2xkRLkiRJA2t5\n80T3VFXNBGa2WYMkSZLUlD3RkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYkSZIa\nMkRLkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJE\nS5IkSQ0ZoiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAtSZIkNWSIliRJkhoyREuS\nJEkNGaIlSZKkhgzRkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYkSZIaMkRLkiRJ\nDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0Z\noiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqSJElqyBAtSZIkNWSIliRJkhoyREuSJEkNGaIl\nSZKkhgzRkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJasgQLUmSJDVkiJYkSZIaMkRLkiRJDRmiJUmS\npIb6HqKTbJbk0iQ3Jbkxydv7XYMkSZK0MlZr4ZjzgXdW1TVJngbMSnJxVd3UQi2SJElSY33via6q\nX1bVNd3HDwA/Bjbtdx2SJEnSimp1THSSLYHdgCvbrEOSJElqIlXVzoGTScBlwN9X1deXsP9Y4FiA\njTfeeI8ZM2b0ucKVM2tW2xVMLNttN5dJkyb1/bie5/7yPA+/ts4xeJ77yZ/liaHNn+cVtd9++82q\nqmmjeW4rITrJ6sB5wLer6qPLe/60adPq6quv7n1hYyhpu4KJ5dJLZzJ9+vS+H9fz3F+e5+HX1jkG\nz3M/+bM8MbT587yikow6RLcxO0eAfwV+PJoALUmSJI03bYyJfgHw58CLkszufr20hTokSZKkFdL3\nKe6q6grACyqSJEkaWK5YKEmSJDVkiJYkSZIaMkRLkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0\nJEmS1JAhWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJ\nktSQIVqSJElqyBAtSZIkNWSIliRJkhoyREuSJEkNGaIlSZKkhgzRkiRJUkOGaEmSJKkhQ7QkSZLU\nkCFakiRJasgQLUmSJDVkiJYkSZIaMkRLkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAh\nWpIkSWrIEC1JkiQ1ZIiWJEmSGjJES5IkSQ0ZoiVJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQIVqS\nJElqyBAtSZIkNWSIliRJkhoyREuSJEkNGaIlSZKkhgzRkiRJUkOGaEmSJKkhQ7QkSZLUkCFakiRJ\nasgQLUmSJDVkiJYkSZIaMkRLkiRJDRmiJUmSpIYM0ZIkSVJDhmhJkiSpIUO0JEmS1JAhWpIkSWqo\nlRCd5MAkNyf5vyR/00YNkiRJ0orqe4hOsirwKeAgYApweJIp/a5DkiRJWlFt9EQ/F/i/qrqtqh4F\nZgAvb6EOSZIkaYWs1sIxNwXuHLF9F7DXsl5w8803M3369EXaDj74YP7qr/4K4En7xsN+OBj4q+5j\n9/d6//HHz2Hdddd9Ym8fz/94+PwTZf/C8+zP9/DuP/74ORxxxBGe36HfP62z1/M71PuPP/74Rf5v\nhvbz2fL/foxeqmql3qDxAZNDgQOr6uju9p8De1XVWxd73rHAsd3N7YCb+1qoBs0GwL1tF6Ge8zwP\nP8/xxOB5nhgG8TxvUVUbjuaJbfRE/wLYbMT25G7bIqrqTODMfhWlwZbk6qqa1nYd6i3P8/DzHE8M\nnueJYdjKtr40AAAWVElEQVTPcxtjon8EbJtkqyRrAIcB57ZQhyRJkrRC+t4TXVXzk7wV+DawKvD5\nqrqx33VIkiRJK6qN4RxU1QXABW0cW0PLoT8Tg+d5+HmOJwbP88Qw1Oe57zcWSpIkSYPOZb8lSZKk\nhgzRkiRJUkOtjImWJGmhJLsD+wAFfL+qrmm5JEljJMka3RWqh45joiWNa0m2qqqfLa9NgynJScAr\nga93m14BfK2qPtBeVRorSb5J55ejJaqql/WxHPVYkpnAUVV1e3f7ucBnq2rXNuvqFUO0BlaSVwIX\nVtUDSf4O2B34gL1YwyXJNVW1+2Jts6pqj7Zq0thJcjOwa1U93N1eC5hdVdu1W5nGQpIXLmt/VV3W\nr1rUe0leApwOfBzYFDgIOHpY/192OIcG2Xur6mtJ9gH+CPgI8C/AXu2WpbGQZHtgR+DpSQ4ZsWsd\nYM12qlIP3E3nfD7c3X4KS1jFVoNpZEju/oK0eVXd3GJJ6qGq+naS44CL6Sz3vVtV/arlsnrGGws1\nyBZ0//xj4MyqOh9Yo8V6NLa2Aw4G1gX+ZMTX7sAxLdalsXUfcGOSLyb5AnADMCfJx5N8vOXaNEaS\n/AkwG7iwuz01iasVD5kk7wU+AewLnALMTPLHrRbVQw7n0MBKch6dHqsD6ASrPwBXDevYq4kqyfOq\n6gdt16HeSPK6Ze2vqrP6VYt6J8ks4EXAzKrardt2fVXt3G5lGktJPgb8bVX9obu9BfC5qjqg3cp6\nwxCtgZXkqcCBwPVVdWuSZwE7V9VFLZemMdTtnXzSP1RV9RctlCNpBST5YVXtneTaESH6uqrape3a\npBXlmGgNrKp6KMmlwGbdKbKgMwZLw+W8EY/XBP6UzjhaDYEkP2PJvyRt3UI56p0bk7wGWDXJtsBf\nAv/Tck0aI0k+VlXHL202lmGdhcWeaA2sJO8HjgJ+yhM/tFVVL2qtKPVcklWAK6rq+W3XopWXZP0R\nm2vSme7uGVV1UkslqQe6Vw7fA7wYCPBt4P0LZ2XRYEuyR1XNWtpsLMM6C4shWgOrOzXWzsM6ibuW\nLMl2wPlV9ey2a1FvOIWhNHiSrAp8qape23Yt/eJwDg2yG+jM3HBP24Wod5I8QOdKQ7p//gr461aL\n0pgZMRQLOjNGTcP/m4aGi61MHFW1IMkWw7xC4eL8h0qD7IPAtUluAB5Z2Og/ysOlqp7Wdg3qqX8a\n8Xg+cDvwqnZKUQ+c1v3zEOCZwJe724cDv26lIvXSbcD3u9MXPriwsao+2l5JveNwDg2sJDcCnwGu\nBx5b2D6sY68msiQvozPvKHSmyDpvWc/X4Eiy5uLjYpM8o6p+11ZNGntJrq6qactr02BLcvISmquq\nTu17MX1gT7QG2UNV5WIMQy7Jh4A9ga90m96e5PlV9e4Wy9LY+c8kL6+q+QBJngmcDzgmerisnWTr\nqroNIMlWwNot16Sxd1NVfW1kQ5JXtlVMr9kTrYGV5KN0hnGcy6LDOa5prSiNuSTXAVOr6rHu9qrA\ntc4vOxySHAO8FDgU2IzOz/NfOd/7cElyIHAmncv9AbYAjvU8D5ck11TV7strGxb2RGuQ7db9c+8R\nbUVnVSwNl3WBhZf3n95mIRpbVfXZJGsA/wVsCbyxqpw/eMhU1YXd+aG37zb9pKoeWdZrNDiSHETn\nl+FNk4y8QrwOnXsdhpIhWgOrqvZruwb1xcIbSC+l04O1L/A37ZaklZXkhJGbwObAbGDvJHsP641I\nE1WS1YE3MuLehiSfqap5LZalsXM3cDXwMmDWiPYHgHe0UlEfOJxDAy3JHwM70lmkAYBhvYFhIusu\n6b4nnSsNP6qqX7VcklbSUm5AelxVva9ftaj3knwOWB04q9v058CCqjq6vao01pKsPpF+MbInWgMr\nyRnAU4H9gM/RGVN5VatFqVeeB+xDJ0SvBnyj3XK0shYPyUkmddvntlORemzPqtp1xPYlSf63tWrU\nK1sm+SAwhUU7t7Zur6TeWaXtAqSV8PyqOhL4ffc/5OcBz2m5Jo2xJJ8GjqMzleENwBuTfKrdqjRW\nkuyU5FrgRuDGJLOS7Nh2XRpzC5Jss3AjydbAghbrUW98AfgXOuOg9wO+xBNzgw8de6I1yBbOLftQ\nkk2A3wLParEe9caLgB2qO/YsyVl0ApeGw5nACVV1KUCS6cBngee3WZTG3InApUlGzs7x+nZLUg+s\nVVXfTZKq+jlwSpJZwEltF9YLhmgNsm8mWRf4CHANnUv9n223JPXA/9G56ezn3e3Num0aDmsvDNAA\nVTUzifMHD5lusNoW2K7bdLOzcwylR5KsAtya5K3AL4BJLdfUM95YqIHU/SHde+FUWEmeAqxZVfe1\nW5nGWpLL6NxUuHC8+5507gK/D1zmfdAl+QadX4L/rdt0BLBHVf1pe1VprCQ5ZFn7q+rr/apFvZdk\nT+DHdKYlfT+dKUn/sap+2GphPWKI1sBKcm1V7bb8Z2qQJXnhsva7zPtgS7Ie8D46N44CXA6cUlW/\nb68qjZUkj9GZunD2wqYRu6uq/qL/VUljwxCtgZXkNOAHwNfLv8hDLcnGdHqgAa6qqnvarEdjL8nT\n6IQqZ+cYIkleARwGPBv4b+DsqnI41pBJcu6y9g/rFUNDtAZWkgeAtencBfwwnR6Oqqp1Wi1MYyrJ\nq+iMe59J5xz/P+DEqjqnzbo0NpLsTOcO/md0m+4FXldVN7RXlcZad5z7y4FXA+sD7/Eq0vBI8hvg\nTuBs4EoWveIwtFcMvbFQA6uqntZ2DeqL99CZY/YegCQbAt8BDNHD4TM8eXaOM3F2jmHzMJ37GO6n\nMzPHmst+ugbMM4EDgMOB1wDn07nqMNQzKRmiNbCS7L6E5vuAn1fV/H7Xo55ZZbHhG7/FOe6HibNz\nDLEkL6IznOO5dH75Pb2qrm63Ko21qloAXAhc2L3R/3A6S7u/r6o+2W51veNwDg2sJD8EdqezCAfA\nznQW43g68Kaquqit2jR2knwE2IXOZULoXA6+rqr+ur2qNFacnWO4dW8svA64gs40pIuEjqr6yzbq\n0tjrhuc/phOgtwTOBT5fVb9os65eMkRrYCX5OvDehZeLkkwBTgXeRedmw6lt1qex050m6/HZG6rK\nZb+HhLNzDLckr1vW/qo6q1+1qHeSfAnYCbgAmDFR7mkwRGtgJbmhqnZaUluS2YbowZdkVeA7VbVf\n27VIkpase8Xhwe7myGA51Df8OyZag+zGJP8CzOhuvxq4qXtJaV57ZWmsVNWCJI8leboL6QyXJN9k\nsUv7Iw3rlFjSMKqqCXmfij3RGlhJ1gLezBOXgb8PfJrOXeBPdb7Z4ZDkv4HdgIt5oqfDsZQDzkV0\nJA06Q7SkcW1pYyodSylJapMhWgMnyVer6lVJrmcJl4OrapcWylIPdeeGpqp+03YtkkYvyceXtd8r\nShpkhmgNnCTPqqpfJtliSfur6uf9rkljL0mAk4G30pkXOnRWp/xEVZ3aZm2SRifJo3SmHv0qcDdP\nXsnOK0pDpvt/87ZV9Z3usMvVquqBtuvqBUO0hkKSDYDfln+hh0aSE4CDgGOr6mfdtq2BfwEurKp/\nbrM+ScuXZH3glXRu/J4P/AdwTlXNabUw9USSY4BjgWdU1TZJtgXOqKr9Wy6tJwzRGjhJ9gY+BPwO\neD+dRRo2oNNbeWRVXdhieRojSa4FDqiqexdr3xC4qKp2a6cyjaWlzNJxH3A18Jmqerj/VakXkkym\ns3rhCcBfV9W/LeclGjBJZtNZnfLKhf9GJ7m+qnZut7LecIo7DaJPAu+mszLhJcBBVfXDJNvTWdXO\nED0cVl88QENnXHSS1dsoSD1xG7Ahi65I+QDwHOCzwJ+3VJfGUJLd6axkdwDwLWBWuxWpRx6pqkc7\no/EgyWosYyrLQWeI1iBabeGS3klOraofAlTVTxb+4GooPLqC+zRYnl9Ve47Y/maSH1XVnklubK0q\njYkkp9JZCvrHdOb0/9uqmt9uVeqhy5K8G1gryQF0pqH9Zss19YwhWoPosRGP/7DYvqH9jXcC2jXJ\n/UtoD7Bmv4tRz0xKsnlV3QGQZHNgUnefvywNvr8Dfgbs2v36h25nx8KV7JxNabj8DfAG4HrgjXSW\nAf9cqxX1kGOiNXCSLKCz6EaAtYCHFu4C1qwqL/VLAyLJS4EzgJ/S+Rneik7v1UzgmKr6WHvVaWUt\nbRalhZxNSYPMEC1JalWSpwDbdzdv9mbC4ZHkoqp6cdt1qLeWtm7DQsN6xcHhHJKktu0BbEnn/6Rd\nk1BVX2q3JI2RDdsuQH1xcNsFtMEQLUlqTZJ/A7YBZgMLus0FGKKHw9OTHLK0nVX19X4Wo96YqMNy\nDNGSpDZNA6a4UNLQejqdXsolTZ1UgCF6iCR5gKXP+/7Oqrqt/1X1jiFaktSmG4BnAr9suxD1xM+r\n6i/aLkJ98zHgLuDf6fzidBidK03XAJ8HprdWWQ94Y6EkqTVJLgWmAlcBjyxsr6qXtVaUxkySa11d\ndOJI8r9VtetibbOrauqS9g06e6IlSW06pe0C1FOLrDiZZH1gX+COqnLVwuHzUJJXAed0tw8FFs62\nM3S9tvZES5KknkhyHvA3VXVDkmfRuax/NZ1L/Gc6D/hwSbI1cDrwPDqh+YfAO4BfAHtU1RUtljfm\nDNGSpL5LckVV7bOEG5EWrmS3TkulaQwlubGqduw+fjewfVUdmeRpwPeHdf5gTQwO55Ak9V1V7dP9\n82lt16Kemjfi8f7AZwGq6oEkj7VTknolyYbAMTwx7zsAw3pzqSFaktR3SZ6xrP1V9bt+1aKeujPJ\n2+jM2LA7cCFAkrWA1dssTD3x38DlwHd4Yt73oWWIliS1YRadYRwBNgd+3328LnAHsFV7pWkMvQE4\nFfgj4NVVNafbvjfwhdaqUq88tar+uu0i+mWVtguQJE08VbVVVW1Np8fqT6pqg6pan87CHBe1W53G\nSlXdU1XHVdXLq+qiEe2X0pkfXMPlvCQvbbuIfvHGQklSa5JcX1U7L69NwyfJHVW1edt1aOx0bxRe\nm86c7/MY8huFHc4hSWrT3Un+Dvhyd/u1wN0t1qP+WdJS4BpgE+1GYUO0JKlNhwMnA9/obn+v26Yh\nsIwbSIMhemgk2b6qfpJk9yXtr6pr+l1TPzicQ5Ik9USSn/HEDaSLq+64eA24JGdW1bFJLl3C7qqq\nF/W9qD4wREuS+i7JN1nGMsBV9bI+liNJjTmcQ5LUhtO6fx5CZ5aGhWOiDwd+3UpFklZIkj2BO6vq\nV93tI4E/A34OnDKs877bEy1Jak2Sq6tq2vLaJI1fSa4B/qiqfpdkX2AG8DZgKrBDVR3aaoE94jzR\nkqQ2rZ3k8XGxSbaiM0WWpMGx6oje5lcDZ1bVf1bVe4Fnt1hXTzmcQ5LUpncAM5PcRufmsy2AN7Zb\nknohyT7AtlX1hSQbApOq6mdt16UxsWqS1apqPrA/cOyIfUObNYf2g0mSxr+qujDJtsD23aafVNUj\nbdaksZfkZGAasB2d5b5XpzMO/gVt1qUxczZwWZJ7gT8AlwMkeTZwX5uF9ZJjoiVJrUnyVOAEYIuq\nOqYbqLerqvNaLk1jKMlsYDfgmqrardt2XVXt0m5lGitJ9gaeBVxUVQ92255D54rDUM4TbU+0JKlN\nXwBmAc/rbv8C+BpgiB4uj1ZVJSmAJI57HzJV9cMltN3SRi394o2FkqQ2bVNV/wjMA6iqh3Alu2H0\n1SSfAdZNcgzwHeCzLdckrRR7oiVJbXo0yVp0F15Jsg3gmOghU1WnJTkAuJ/OuOiTqurilsuSVopj\noiVJrekGq78DpgAX0bnR7KiqmtlmXZK0PIZoSVKrkqwP7E1nGMcPq+relkvSGEnyAEte3j1AVdU6\nfS5JGjMO55Akte2FwD50wtbqwDfaLUdjpaqe1nYNUq/YEy1Jak2ST9NZ0ezsbtOrgZ9W1Vvaq0q9\nkGR3nvhl6YqqurblkqSVYoiWJLUmyU+AHar7n1GSVYAbq2qHdivTWEpyEvBK4OvdplcAX6uqD7RX\nlbRyHM4hSWrT/wGbAz/vbm/WbdNweS2wa1U9DJDkQ8BswBCtgWWIliT1XZJv0rms/zTgx0mu6m7v\nBVzVZm3qibuBNYGHu9tPobOwjjSwDNGSpDac1nYB6qv7gBuTXEznl6UDgKuSfBygqv6yzeKkFeGY\naElS65Ksw4iOnar6XYvlaIwled2y9lfVWf2qRRorhmhJUmuSHAucSucy/2M8MX/w1q0WJknLYYiW\nJLUmya3A81xgZbglORh4P7AFnSsOLraigeeYaElSm34KPNR2Eeq5jwGHANeXvXcaEoZoSVKb/hb4\nnyRXAo8sbPRGs6FzJ3CDAVrDxBAtSWrTZ4BLgOvpjInWcHoXcEGSy1j0l6WPtleStHIM0ZKkNq1e\nVSe0XYR67u+BuXTmil6j5VqkMWGIliS16VvdGTq+yaI9lE5xN1w2qaqd2i5CGkvOziFJak2Sny2h\n2SnuhkySfwS+U1UXtV2LNFYM0ZIkqaeSPACsTedqwzyc4k5DYJW2C5AkTTxJ3jXi8SsX2/cP/a9I\nvVRVT6uqVapqrapap7ttgNZAsydaktR3Sa6pqt0Xf7ykbQ2uJNtX1U+SLPF8VtU1/a5JGiveWChJ\nakOW8nhJ2xpcJwDHAv+0hH0FvKi/5UhjxxAtSWpDLeXxkrY1oKrq2O6f+7VdizTWHM4hSeq7JAuA\nB+n0Oq/FE0t/B1izqlZvqzaNnSR7AndW1a+620cCfwb8HDjFqQw1yAzRkiSpJ5JcA/xRVf0uyb7A\nDOBtwFRgh6o6tNUCpZXgcA5JktQrq47obX41cGZV/Sfwn0lmt1iXtNKc4k6SJPXKqkkWdtjtD1wy\nYp8deRpo/gWWJEm9cjZwWZJ7gT8AlwMkeTZwX5uFSSvLMdGSJKlnkuwNPAu4qKoe7LY9B5jkPNEa\nZIZoSZIkqSHHREuSJEkNGaIlSZKkhgzRkjQOJakkXx6xvVqS3yQ5r+H73J5kg5V9jiRpUYZoSRqf\nHgR2SrJWd/sA4Bct1iNJGsEQLUnj1wXAH3cfH05nujAAkjwjyX8luS7JD5Ps0m1fP8lFSW5M8jk6\ny2gvfM0RSa5KMjvJZ5KsOvJgSdZOcn6S/01yQ5JX9/4jStJgMkRL0vg1AzgsyZrALsCVI/a9D7i2\nqnYB3g18qdt+MnBFVe0IfAPYHCDJDnRWjHtBVU0FFgCvXex4BwJ3V9WuVbUTcGFvPpYkDT4XW5Gk\ncaqqrkuyJZ1e6AsW270P8Gfd513S7YFeB9gXOKTbfn6S33efvz+wB/CjJABrAfcs9p7XA/+U5MPA\neVV1+Zh/KEkaEoZoSRrfzgVOA6YD66/E+wQ4q6r+dmlPqKpbkuwOvBT4QJLvVtWpK3FMSRpaDueQ\npPHt88D7qur6xdovpzscI8l04N6quh/4HvCabvtBwHrd538XODTJRt19z0iyxcg3TLIJ8FBVfRn4\nCLB7Tz6RJA0Be6IlaRyrqruAjy9h1ynA55NcBzwEvK7b/j7g7CQ3Av8D3NF9n5uS/B1wUZJVgHnA\nW4Cfj3jPnYGPJHmsu/9NY/+JJGk4uOy3JEmS1JDDOSRJkqSGDNGSJElSQ4ZoSZIkqSFDtCRJktSQ\nIVqSJElqyBAtSZIkNWSIliRJkhoyREuSJEkN/X+cfhQt9K+xSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff42f71d310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def plotSuccess():\n",
    "    s = pd.Series(methodDict)\n",
    "   \n",
    "    \n",
    "    # Colors\n",
    "    ax = s.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    plt.ylim([0.0, 10.0])\n",
    "    plt.xlim([-0.5, 5])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Success of methods')\n",
    "     \n",
    "    plt.show()\n",
    "plotSuccess()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
