{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.330120\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 10: 9.775137\n",
      "Minibatch accuracy: 36.7%\n",
      "Validation accuracy: 42.0%\n",
      "Minibatch loss at step 20: 7.126841\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 53.1%\n",
      "Minibatch loss at step 30: 7.193448\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 62.1%\n",
      "Minibatch loss at step 40: 6.384779\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.4%\n",
      "Minibatch loss at step 50: 5.853183\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 60: 6.382040\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 70: 5.899903\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 80: 4.949805\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 90: 4.559985\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 100: 4.715533\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 110: 5.176573\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 120: 5.315533\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 130: 5.127513\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 140: 3.729640\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 150: 5.208105\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 160: 5.385163\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 170: 4.784753\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 180: 4.222723\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 190: 4.194992\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 200: 4.385221\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 210: 3.781316\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 220: 3.784232\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 230: 4.203263\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 240: 3.906820\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 250: 3.337084\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 260: 3.940071\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 270: 3.636664\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 280: 3.723283\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 290: 3.898583\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 300: 3.450459\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 310: 3.753567\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 320: 3.699995\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 330: 3.455368\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 340: 3.698577\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 350: 3.454812\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 360: 3.446204\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 370: 2.724008\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 380: 3.206986\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 390: 3.323558\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 400: 3.299488\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 410: 3.007625\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 420: 3.039844\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 430: 3.335303\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 440: 3.551635\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 450: 2.661637\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 460: 2.819670\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 470: 2.933730\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 480: 2.517828\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 490: 2.787542\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 500: 2.309772\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 510: 2.437473\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 520: 2.727597\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 530: 2.549912\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 540: 2.933038\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 550: 2.591496\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 560: 2.419780\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 570: 2.669944\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 580: 2.995780\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 590: 2.667299\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 600: 2.789630\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 610: 2.299593\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 620: 2.415045\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 630: 2.426963\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 640: 2.591332\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 650: 2.290125\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 660: 2.494729\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 670: 2.185501\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 680: 2.310090\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 690: 2.252035\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 700: 2.429035\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 710: 2.155380\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 720: 2.455751\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 730: 2.432403\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 740: 2.135453\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 750: 2.306924\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 760: 2.320788\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 770: 2.179179\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 780: 2.058278\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 790: 1.769717\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 800: 2.183186\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 810: 2.039409\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 820: 1.866349\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 830: 2.562193\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 840: 2.183181\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 850: 1.833700\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 860: 1.791850\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 870: 2.490385\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 880: 1.923015\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 890: 1.632022\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 900: 1.793002\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 910: 1.983430\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 920: 1.820299\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 930: 1.949133\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 940: 1.514700\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 950: 1.580204\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 960: 1.770784\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 970: 1.952528\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 980: 2.009579\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 990: 1.778211\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1000: 1.822701\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1010: 1.632628\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1020: 1.654886\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1030: 1.916762\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1040: 1.668523\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1050: 1.781748\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1060: 1.383577\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1070: 1.435731\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1080: 1.704832\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1090: 1.627178\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1100: 1.407999\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 1110: 1.602002\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1120: 1.674744\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1130: 1.545171\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1140: 1.430001\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1150: 1.519414\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1160: 1.711329\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1170: 1.496286\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1180: 1.391534\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1190: 1.336584\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1200: 1.508523\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1210: 1.588276\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1220: 1.408894\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1230: 1.803421\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1240: 1.364327\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1250: 1.391727\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1260: 1.394808\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1270: 1.586684\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1280: 1.680668\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1290: 1.395953\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1300: 1.530941\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1310: 1.663123\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1320: 1.571594\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1330: 1.220477\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1340: 1.116641\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1350: 1.339859\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1360: 1.198289\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1370: 1.166101\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1380: 1.823883\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1390: 1.478193\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1400: 1.431535\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1410: 1.235120\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1420: 1.079273\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1430: 1.382583\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1440: 1.407777\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1450: 1.075934\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1460: 1.051433\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1470: 1.292796\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1480: 1.306188\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1490: 1.305225\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 1.249400\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1510: 1.160083\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1520: 1.063782\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1530: 1.141724\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1540: 1.123309\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1550: 1.229372\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1560: 1.208489\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1570: 1.229501\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1580: 1.218992\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1590: 1.214707\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1600: 1.155731\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1610: 1.099469\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1620: 1.327955\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1630: 1.180921\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1640: 0.817713\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1650: 1.241551\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1660: 0.851565\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1670: 0.997037\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1680: 0.756625\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1690: 1.092127\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1700: 1.060043\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1710: 1.205130\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1720: 1.228140\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1730: 1.028135\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1740: 1.245958\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1750: 1.226252\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1760: 0.995587\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1770: 1.018141\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1780: 1.348594\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1790: 1.016086\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1800: 1.056730\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1810: 1.105545\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1820: 0.897201\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1830: 1.107748\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1840: 1.026561\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1850: 1.217972\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1860: 1.253133\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1870: 1.145747\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1880: 0.900800\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1890: 0.931318\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1900: 1.036251\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1910: 0.996863\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1920: 1.027329\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1930: 0.787941\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1940: 1.028545\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1950: 0.924841\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1960: 0.847942\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1970: 1.060444\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1980: 0.985799\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1990: 0.810295\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.883508\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2010: 0.762982\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2020: 0.997401\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2030: 0.757251\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2040: 0.842859\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2050: 1.033883\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2060: 1.150856\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2070: 0.896222\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2080: 0.954331\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2090: 0.806420\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2100: 0.969541\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2110: 0.817706\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2120: 0.934635\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2130: 0.766337\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2140: 0.672586\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2150: 1.087000\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2160: 0.872587\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2170: 0.955377\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2180: 0.961939\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2190: 1.008812\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2200: 0.739348\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2210: 0.921194\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2220: 1.021922\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2230: 0.749611\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2240: 0.936402\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2250: 0.872153\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2260: 0.985609\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2270: 0.796664\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2280: 0.990365\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2290: 0.738447\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2300: 0.955488\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2310: 0.941531\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2320: 0.794609\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2330: 0.711546\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2340: 0.639087\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2350: 1.212397\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2360: 0.791703\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2370: 0.949488\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2380: 0.751464\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2390: 1.026491\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2400: 0.951290\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2410: 0.963836\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2420: 0.846757\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2430: 0.793220\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2440: 0.986108\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2450: 0.891302\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2460: 0.739269\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2470: 0.840972\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2480: 1.028146\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2490: 0.849189\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.923967\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2510: 0.763660\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2520: 0.737776\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2530: 0.641336\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2540: 0.862097\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2550: 1.042191\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2560: 0.717979\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2570: 0.602824\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2580: 0.916700\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2590: 1.025030\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2600: 0.764840\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2610: 0.879870\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2620: 0.856033\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2630: 0.779512\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2640: 0.793535\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2650: 0.944226\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2660: 0.831168\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2670: 0.757003\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2680: 0.801562\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2690: 0.736228\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2700: 0.817741\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2710: 0.790161\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2720: 0.639351\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2730: 0.835953\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2740: 0.761416\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2750: 0.650085\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2760: 0.710566\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2770: 0.789869\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2780: 0.791250\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2790: 0.831542\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2800: 0.760865\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2810: 0.579534\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2820: 0.730632\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2830: 0.614656\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2840: 1.072925\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 2850: 0.826100\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2860: 0.800361\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2870: 0.905965\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2880: 0.651997\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2890: 1.061980\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2900: 0.753061\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2910: 0.762541\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2920: 0.867904\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2930: 0.905101\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2940: 0.668987\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2950: 0.730228\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2960: 1.027314\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2970: 0.693868\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2980: 0.803838\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2990: 0.862697\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3000: 0.993636\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "            losses.append(l)\n",
    "            acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting loss and accuracy by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(acc, color='g')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network L2 regularization\n",
    "\n",
    "In L2 regularization there is a new regularization parameter. The right amount of regularization should improve your validation / test accuracy. Let's see what the best value for that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Test Regul./Accuracy: '1.000e-04' 86.24\n",
      "Test Regul./Accuracy: '1.259e-04' 86.46\n",
      "Test Regul./Accuracy: '1.585e-04' 86.95\n",
      "Test Regul./Accuracy: '1.995e-04' 86.66\n",
      "Test Regul./Accuracy: '2.512e-04' 87.49\n",
      "Test Regul./Accuracy: '3.162e-04' 87.45\n",
      "Test Regul./Accuracy: '3.981e-04' 87.6\n",
      "Test Regul./Accuracy: '5.012e-04' 88.17\n",
      "Test Regul./Accuracy: '6.310e-04' 88.56\n",
      "Test Regul./Accuracy: '7.943e-04' 88.53\n",
      "Test Regul./Accuracy: '1.000e-03' 88.82\n",
      "Test Regul./Accuracy: '1.259e-03' 89.1\n",
      "Test Regul./Accuracy: '1.585e-03' 89.18\n",
      "Test Regul./Accuracy: '1.995e-03' 89.18\n",
      "Test Regul./Accuracy: '2.512e-03' 89.16\n",
      "Test Regul./Accuracy: '3.162e-03' 89.11\n",
      "Test Regul./Accuracy: '3.981e-03' 89.07\n",
      "Test Regul./Accuracy: '5.012e-03' 89.03\n",
      "Test Regul./Accuracy: '6.310e-03' 89.0\n",
      "Test Regul./Accuracy: '7.943e-03' 88.89\n",
      "Ended\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "\n",
    "print(\"Initialized\")\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))  \n",
    "    \n",
    "    print(\"Test Regul./Accuracy: '%.3e'\" % regul, accuracy_val[-1])\n",
    "print(\"Ended\")            \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting regularization best value parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAH0CAYAAAADoIroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAADxC0AQAAAA/8P+rQbWUey7jnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9f9082dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Regularitazion')\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization\n",
    "\n",
    "The idea is to add another term to the loss, which penalizes large weights.\n",
    "It's typically achieved by adding the L2 norm of your weights to the loss, multiplied by a small constant.\n",
    "\n",
    "Let's try now adding this new parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 614.929260\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 27.8%\n",
      "Minibatch loss at step 500: 192.081665\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1000: 116.037239\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1500: 69.272758\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 41.182613\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2500: 25.212254\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3000: 15.549626\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.4%\n",
      "Test accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "            losses.append(l)\n",
    "            acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plotting loss and accuracy by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAHjCAYAAADojTN7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9f8bd4910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAHjCAYAAADlk0M8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nES1JkiTNySJakiRJmpNFtCRJkjSnfwEsJWGXEek2vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9f8a7a450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(acc, color='g')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the L2 parameter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Test Regul./Accuracy: '1.000e-04' 88.54\n",
      "Test Regul./Accuracy: '1.259e-04' 88.86\n",
      "Test Regul./Accuracy: '1.585e-04' 88.47\n",
      "Test Regul./Accuracy: '1.995e-04' 88.71\n",
      "Test Regul./Accuracy: '2.512e-04' 88.96\n",
      "Test Regul./Accuracy: '3.162e-04' 90.14\n",
      "Test Regul./Accuracy: '3.981e-04' 90.28\n",
      "Test Regul./Accuracy: '5.012e-04' 90.84\n",
      "Test Regul./Accuracy: '6.310e-04' 91.77\n",
      "Test Regul./Accuracy: '7.943e-04' 92.61\n",
      "Test Regul./Accuracy: '1.000e-03' 92.97\n",
      "Test Regul./Accuracy: '1.259e-03' 93.75\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "\n",
    "print(\"Initialized\")\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))  \n",
    "    \n",
    "    print(\"Test Regul./Accuracy: '%.3e'\" % regul, accuracy_val[-1])\n",
    "print(\"Ended\")            \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Problem 1\n",
    "\n",
    "We've got improved Test Accuracy from 89.1% with Logistic Regressioon to 93.2% with a 1-layer Neural Network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "            losses.append(l)\n",
    "            acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion Problem 2\n",
    "\n",
    "The generalization capability is poor, as shown in the validation and test accuracy. Since there are far too much parameters and no regularization, the accuracy of the batches is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    # tf.nn.dropout(<A floating point tensor>, <A scalar Tensor with the same type as x. The probability that each element is kept>)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "valid_acc = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = step % num_batches\n",
    "        offset = step % num_batches\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "            losses.append(l)\n",
    "            acc.append(accuracy(predictions, batch_labels))\n",
    "            valid_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
